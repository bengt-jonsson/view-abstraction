\section{Overview}
\label{sec:overview}

In this section, we illustrate our technique by proving correctness of
a concurrent data structure implementation.

\subsection{The Algorithm}

\input tscode

\todo[inline]{Provide a description of the algorithm}

\todo[inline]{We may need to assume that programs are data-independent}

\subsection{Specifying Correctness}
In this subsection, we describe how we specify that the algorithm in Figure XXX
is correct in the sense that it is a linearizable implementation of a
stack data structure. Linearizability intuitively states that
each operation on the data structure can be considered as being
performed atomically at some point, called the {\em linearization point (LP)},
between its invocation and return.
For many data structure implementations, LPs can be statically
affixed to particular statements in method implementations (so-called
{\em fixed LPs}),
implying that correctness can be formulated as constraints on the ordering of the
occurrences of linearization points of the methods in any program
execution.
\bjcom{Give list of works}
In our work, we use this technique to specify linearizability of
set implementations (see Section XXX).
%% a concept of data-independence, these constraints can be formulated as
%% a number of samll observers, which are automata that recognize violations
%% of these ordering requirements~\cite{AHHR:integrated}.

Unfortunatly, in the TS Stack algorithm of Figure XXX, LPs cannot be affixed
to particular statements. For instance,  two overlapping
push operations may have to be linearized in different orders depending
on how their corresponding later pop operations are ordered in time.
%% not be determined during their invocation, but only later
%% when the ordering between the two corresponding pop operations is determined.
Fortunately, Bouajjanni etal.\ recently established that for stacks and
queues, linearizability can be precisely specified without refering to
linearization points~\cite{BEEH:icalp15}. They showed to linearizability
of concurrent stack implementations can be precisely described by
a small number of simple 
constraints on the ordering of call and return actions.
Let us describe how to represent and verify these constraints.

Let a call action on an operation $\mname(\data)$ be denoted
$\calltwo{\mname}{\data}$ and a return operation be denoted
$\returntwo{\mname}{\data}$. Ordering constraints on the sequence of
call and return actions for a small set of data values $\data$ can be
checked by an {\em observer},
as introduced in~\cite{AHHR:integrated}. 
%
Observers are
finite automata extended with a finite set of {\em registers}
that assume values in $\intgrs$. 
%
At initialization,
the registers are nondeterministically
assigned arbitrary values, which never change
during a run of the observer. 
%
%% Formally, an observer $\observer$ is a tuple
%% $\otuple$ where $\ostateset$ is a finite set 
%% of {\it observer states} including the 
%% {\it initial state} $\oinitstate$ and
%% the {\it accepting state} $\oaccstate$, 
%% a finite set $\ovarset$  
%% of {\it registers}, and $\otransitionset$ is a finite
%% set of {\it transitions}.
%% %
%% %
%% Transitions are of the form 
%% $\tuple{\ostate_1,\mname(\inxvar,\outxvar),\ostate_2}$ where 
%% $\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
Transitions are labeled by 
operations whose input or output data may be parameterized on registers.
%
%% The observer processes a trace one operation at a time.
%% %
%% If there is a transition, whose label, after replacing registers by their
%% values, matches the operation, such a transition is performed. 
%% %
%% If there is no
%% such transition, the observer remains in its current state.
%
The observer accepts a trace if it can  be processed in such a way that
an accepting state is reached.
%
The observer is defined in such a way that it accepts precisely those
traces that do {\em not} belong to the behavior
of the data structure.
As an example, Figure ~\ref{fig:nonpopobserver:fig} shows an observer which intuitively specifies that 
a data item must not be popped before it is popped. More precisely, it
specifies that the pop operation must not return before any push operation
for the same data value has been called.

\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};

\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(80pt,0pt)$) {$s_1$};


\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(80pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\footnotesize${\tt ret \; pop}(x)$} 
(s1);

\draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s0) to 
node[above=-15pt,pos=0.5] {\footnotesize${\tt call \; pop}(x)$} 
(s0);



\end{tikzpicture}
\caption{Non pop without push observer.}
\label{fig:nonpopobserver:fig}
\end{figure}

%\todo[inline]{Quy: make a picture of this observer, and describe intuitively
%  how it works}


In~\cite{BEEH:icalp15}, is it shown that
a stack can precisely specified by four observers,
that specify the following intuitive constraints.
%\todo[inline]{Quy: Make sure that you have the right constraints. I just guessed that the below four are the right ones}
\begin{enumerate}
\item a data item must not be popped before it is pushed,
\item pop must not return ``empty'' if a data element was pushed but not
  popped
\item a pushed data item must not be popped twice
\item a data item must be popped with respect to last in first out stack order% \bjcom{Here we specify stack order}
\end{enumerate}
The exact form of observers for these constraints must represent them in
terms of ordering of call an return actions. Observers for constraint
2 and 3 are similar to that for constraint 1. The observer for constraint
4 is more complex, and shown in Figure ~\ref{fig:lifostack:fig}.
%\bjcom{Quy: Insert the observer, and an explanation}
\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};


\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(60pt,0pt)$) {$s_1$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(120pt,0pt)$) {$s_2$};

\node(s3)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(180pt,0pt)$) {$s_3$};

\node(s4)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,0pt)$) {$s_4$};

\node(s6)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(300pt,0pt)$) {$s_6$};

\node(s5)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,-50pt)$) {$s_5$};

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(300pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; push}(x_1)$} 
(s1);
\draw[->,>=stealth,line width=0.5pt,] (s1) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_1)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,] (s2) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s3);

\draw[->,>=stealth,line width=0.5pt,] (s3) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; pop}(x_1)$} 
(s4);

\draw[->,>=stealth,line width=0.5pt,] (s4) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; pop}(x_1)$} 
(s6);

\draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s2) to 
node[above=-15pt,pos=0.5] {\tiny${\tt call \; push}(x_2)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,out=-35,in=35] (s4) to 
node[right=2pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s5);

\draw[->,>=stealth,line width=0.5pt,out=145,in=-145] (s5) to 
node[left=2pt,pos=0.5] {\tiny${\tt call \; pop}(x_2)$} 
(s4);

\end{tikzpicture}
\caption{LIFO stack order observer.}
\label{fig:lifostack:fig}
\end{figure}

\subsection{Verification by Fragment Abstraction}
To verify that TS-Stack is a correct linearizable implementation of a stack, we
must show that any concurrent execution of method calls satisfies the
above four constraints (1) -- (4).
To this end, define a {\em program} to consist 
of an arbitrary number of concurrently executing threads,
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
each of which executes a push or pop method with some data value as parameter.
We assume that the data structure has been initialized
by the {\tt init} method prior to the start of program execution.
We must verify that any execution of such a program satisfies the four
constraints above. To verify a property, 
we form, as in the automata-theoretic approach~\cite{VW:modelchecking},
also adopted in~\cite{AHHR:integrated},
the cross-product of the program  and the corresponding
observer, where the observer synchronizes with the program on call and
return actions that are recognized by the observer. This reduces the
problem of checking each of the above constraints to the problem of checking
that the observer cannot reach an accepting state.

\todo[inline]{We may have to say that we check the trivial conditions that
  were checked by the monitor in SAS 16}
%% Instrumentation to check conditions 
%% (i) - (iii) is added automatically, reducing the verification to a
%% reachability problem, i.e., to verify that no configuration that violates
%% conditions (i) - (iv) can be reached.

%% The addition of observers reduces the problem of checking linearizability to
%% checking control state reachability for the composition of an arbitrary
%% program and an observer.
%% \begin{enumerate}[(i)]
%% \item each method invocation generates a sequence of linearization events,
%%   of which only the last one may change the state of the observer,
%% \item
%%   the linearization event which is generated last
%%   is consistent with the call parameters and return value of the method,
%%   and
%% \item  that the sequence of linearization
%%   events cannot cause the observer to reach an accepting state.
%% \end{enumerate}
%% Tasks (i) and (ii) can be verified by standard instrumentations of
%% methods, which we will not further elaborate on here.
%% is added automatically, reducing the verification to a
%%  and our verification by standard techniques, as well as
%% the standard way, and verify condition (iv) by checking that the observer cannot
%% reach an accepting state.
In the actual verification, we compute a symbolic representation
of an invariant that is satisfied by all reachable program configurations (including observer states).
This invariant is obtained by an abstract-interpretation-based
fixpoint procedure, which starts
from a representation of the set of initial configurations, and
thereafter repeatedly performs
postcondition computations that extend the
symbolic representation by the effect of any execution step of the program,
until convergence.
This analysis needs to deal with the challenges of an unbounded domain of
data values, an unbounded number of concurrently executing threads, and an
unbounded heap. We have therefore developed a symbolic representation, which
is a careful combination of
data abstraction, thread abstraction, and shape abstraction. 
%% The analysis needs to deal with the challenges of an unbounded domain of
%% data values, an unbounded number of concurrently executing threads, and an
%% unbounded heap. We have therefore developed a technique for verification,
%% which uses a carefully designed combination of
%% data abstraction, thread abstraction, and shape abstraction.
Let us illustrate this symbolic representation on the algorithm in Figure XXX.

Our {\em data abstraction} abstracts from concrete data values in
$\intgrs$ by representing only {\em relations}
(in the set $\set{\prec,\equiv,\succ}$) between values of {\tt Data} fields of 
heap cells, local {\tt Data} variables, and registers of the observer.
We similarly abstract from concrete values of timestamps by representing
only relations between values of timestamps in heap cells and in
local variables.
In order to cope with an unbounded number of threads,
we extend the {\em thread-modular approach}~\cite{BLMRS:cav08}, in which
the symbolic representation characterizes
the projection of the configuration onto a single thread $t$.
%% The postcondition computation must consider how this projection can
%% be changed by execution steps of the program.
%% Such a step can be performed
%% either by the thread $t$ (a local step) or by 
%% some other thread $t'$ (an interference step).
%% Local steps can be handled by adapting standard techniques
%% for computing postconditions.
The corresponding postcondition computation must then take into account that
the projection onto a particular thread can be changed either by step of that
thread, or by a step of another thread.
\bjcom{omit this last sentence?}
%% interference steps involve to first compute a new abstract configuration
%% over two thread identifiers
%% which represents the combined state of both threads $t$ and $t'$,
%% and only therafter
%% performing the step of $t'$; thereafter the resulting abstract post-state
%% is projected back onto $t$.

A main contribution of this work is 
a novel {\em shape abstraction}, which is designed to cope with an unbounded
heap (note that the heap visible to a single thread is still unbounded).
The idea of this abstraction, called {\em fragment abstraction} is
to represent a heap by a set of (heap) {\em fragments}, where each fragment is
just a pair of nodes that are connected by a {\tt next} pointer. Thus, the
representation of a particular heap is obtained as the set of pairs of nodes
that are connected by a {\tt next pointer}. Conversely,
a set of (heap) fragments represents the set of heaps that can be formed by
putting together fragments from this set, where each fragment may be used
multiple times. In order to obtain a representation which is both expressive
and bounded, nodes are first abstracted to a finite set of node types, called
{\em tags}. A tag represents both local information about the values of
fields of the node
and about which pointer variables point to the node, as well as global
information about how the node is positioned in the overall projected heap.
The global information records from which global and thread-local pointer
variables the node can be reached by a sequence of pointers
\todo[inline]{Be careful here in the description: I believe we do not need any
  {\tt reachto} for TS-Stack. Do we need it for TS-Queue?}

Let us illustrate our symbolic representation for the TS-Stack algorithm.
The heap in the algorithm consists of a set of singly linked lists (SLLs). Each
SLL is accessed from a particular pointer {\tt top} in an array of such pointers.
Fig. ~\ref{fig:tsshape} shows an example of a snapshot of the heap state, in a situation when
% Quy write from here
it is accessed concurrently by three threads $\tt T_1$, $\tt T_2$, and $\tt T_3$. The thread $\tt T_1$ is trying to push a new node pointed by its local variable $\tt new1$ while $\tt T_3$ has just called its push method and $\tt T_2$ is doing its pop method. The thread $\tt T_2$ find the youngest node in the list owned by $\tt T_3$ which is determined by its local variable $\tt youngest2$.
%\todo[inline]{Quy: Make a picture of a concrete shape with several threads.
%  Say which operations are working there now, and give some
%  explanation of what the figure says}.
\begin{figure}
	\input tsshape
\caption{A concrete shape of timestamp stack with three threads}
\label{fig:tsshape}
\end{figure} 

To obtain our symbolic representation, we first apply data abstraction and
project the configuration onto a single thread. Fig. ~\ref{fig:tsdabsshape} shows the result
of projecting onto thread 2, which performs a pop with data abstraction. As can be seen the the figure, the node pointed by $\tt new1$ is not visible with $\tt T_2$ because it is still the private variable of $\tt T_1$. We use three observer registers $\tt d_1$, $\tt d_2$, and $\tt d_3$. The data value $\tt 9$ is equal to $\tt d_1$, $\tt 8$ is equal to $\tt d_2$ and all other data values are equal to $\tt d_3$. To abstract timestamp of a node, we keep the ordering between its timestamp and nodes whose data values are equal to $\tt d_1$ or $\tt d_2$ which are described by $\tt to_1$,...,$\tt to_6$ in the figure.
% Quy write here
%\todo[inline]{Quy: make a picture of the corresponding
%  thread view with data abstraction  Complete this description}.
\begin{figure}
	\input tsshapeb
\caption{The projection on thread 2 after data abstraction}
\label{fig:tsdabsshape}
\end{figure} 

\begin{figure}
	\input tsabsshape
\caption{Fragment abstraction}
\label{fig:tsviewshape}
\end{figure} 
In order to represent the projection in Fig. ~\ref{fig:tsdabsshape} by
a set of fragments, we define the information conveyed by node tags.
We define one tag for push operations and one for pop operations, since
their methods use different sets of local variables.
For a pop operation, a {\em tag} is defined as a tuple
$\tuple{\pvars,\data,\reachfrom,\reachto,\private}$, where
\begin{itemize}
\item
  $\pvars$ is a set of pointer variables (i.e., a subset of
  {\tt top, youngest, n, myTop}
  %\bjcom{Quy: Check exactly what this set should be}
\item
  $\vals$ maps
  \begin{inparaenum}[(i)]
  \item
    each {\tt mark} field to a boolean value,
  \item
    each {\tt d} field to a mapping from observer registers to $\set{=, \neq}$,
  \item
    each {\tt time} field to a mapping from each observer register to a subset of $\set{<,=,>}$ %\bjinsert{Quy: insert the correct text here}.
  \end{inparaenum}
\item
  $\reachfrom$ and $\reachto$ are sets of global variables and observer registers, %\bjcom{Quy: Check what is really needed here: Maybe we need only reachfrom?}
\item
  $\private$ is a boolean value.
\end{itemize}
A node $\cell$ in the heap of a thread view satisfies a tag $\tagtuple$,
denoted $\cell \lhd \tagtuple$ if
\begin{itemize}
\item
  $\pvars$ is the set of pointer variables that point to $\cell$.
  %\bjcom{Quy: Or Should it be contains?}
\item
The value of {\tt mark} field of $\cell$ is equal to the value of $\vals\tt {.(mark)}$. For each observer register $\tt d_i$, the relation of {\tt d} field  and $\tt d_i$ is a subset of $\vals\tt {.(d).(d_i)}$, and the relation between value of {\tt time} field and cells whose $\tt d$ field is equal to $\tt d_i$ is a subset of $\vals\tt {.(time).(d_i)}$. 
  %\bjcom{Quy: continue to write here}
\item $\reachfrom$ and $\reachto$ are sets of global variables and observer registers in which the cell is reachable and reach to.
\item $\private$ is the private information of the cell.
\end{itemize}

We can now define the concept of fragment.
A {\em fragment} is a pair $\tuple{\inputtag,\outputtag}$ of tags.
A {\em symbolic configuration} is a pair consisting of a set a value of a program
pointer, observer state, and value of local variables \bjcom{or: local state}, and a set of fragments.
%\todo[inline]{Quy: Could you explain exactly what is in a symbolic configuration?
%  Does it contain some information about values of local variables, such as
%  {\tt success} and {\tt maxTS}?}

%\todo[inline]{Quy: make a picture with all the fragments obtained from the
 % thread projection}
Figure ~\ref{fig:tsviewshape} shows all the fragments obtained from the thread projection from ~\ref{fig:tsdabsshape}. The $\reachfrom$ of each node is shown in the first row under each node, whereas the $\reachto$ is shown in the second row.  
A thread-abstracted configuration satisfies a symbolic configuration if
\begin{itemize}
\item it satisfies program counter, observer state, and local variables %\bjcom{Say that program counter, and local variables ...}
\item
  For each cell $\cell$ in the heap of the configuraion, there is
  a fragment $\tuple{\inputtag,\outputtag}$ in the set of fragments, such
  that $\cell \lhd \inputtag$ and ${\tt next}(\cell) \lhd \outputtag$.
  \bjcom{check the case when $\cell$ points to null etc.}
\end{itemize}

symbolic configurations. 
A {\em symbolic representation} $\symbrep$ is a set of
symbolic configurations. 
A  set of thread-abstracted configurations satisfies a symbolic
representation $\symbrep$ if each of its thread-abstracted
configuration satisfies some symbolic configuration in $\symbrep$.





