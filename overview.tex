\section{Overview}
\label{sec:overview}

In this section, we illustrate our technique by using it to prove correctness of
a concurrent data structure implementation, namely the Timestamped Stack
of~\cite{ts-stack}.

\subsection{The Algorithm}

\input tscode

%\todo[inline]{Provide a description of the algorithm}
Figure~\ref{ts-stack:fig} shows a simplified version of the Timestamped Stack (TS stack) without the emptiness check and elimination. The full version of the algorithm is described in the Appendix XXX. The algorithm uses an array of singly-linked lists (SLLs), one for each thread, which are accessed via an array {\tt pools[maxThreads]} of pointers to the first node of each list, each of which is initially $\nullconst$. Each node of a list contains a data value, a time, the next pointer, and a boolean flag indicating whether it is logically removed from the stack or not.

A Push method for inserting an element {\tt d} into the TS-stack is performed as follows: First, a new node with element {\tt d} and minimal timestamp {\tt -1} is inserted at the beginning of the list which is owned by the calling thread (line 1-3). After that, a new timestamp is created and assigned (via the variable {\tt t}) to the {\tt time} field of the added node (line 4-5).
%\bjcom{Quesion to Quy: What happens when pop methods sees uninitialized timestamps?} \quycom{uninitialized time is -1, its smaller than any other time.}
Finally, the method unlinks all nodes whose {\tt mark} field are equal to {\tt true} and reachable from the added node. The unlink is performed by redirecting the $\tt next$ pointer from the new node to the first node whose $\tt mark$ field is $\tt false$ and previously connected by a sequence of marked nodes (line 7-9).

Popping an element from TS stack consists in traversing all the lists, finding
the first element whose {\tt mark} field is {\tt false} in each
list, and selecting the youngest such node (i.e., with the maximal timestamp) (line 1-11).
A compare-and-swap (CAS) is used
to set the {\tt mark} field of this youngest node to $\true$,
thereby logically removing it.
The procedure will restart if the CAS fails. After the youngest node has been removed, the method will unlink all nodes whose {\tt mark} field are equal to {\tt true} appear before (line 15-16) or after removed  node (line 18-21).
%\todo[inline]{Comments by Bengt: How can you return empty? Also, I think you
 % need to explain the extra removal of marked nodes better} \quycom{this code is a simplified version without emptiness checking. If we add emptiness part, the code will be longer and more complicated}

\subsection{Specifying Correctness}
In this subsection, we describe how we specify that the algorithm in Figure~\ref{ts-stack:fig}
is correct in the sense that it is a linearizable implementation of a
stack data structure. Linearizability intuitively states that
each operation on the data structure can be considered as being
performed atomically at some point, called the {\em linearization point (LP)},
between its invocation and return.
For many data structure implementations, LPs can be statically
affixed to particular statements in method implementations (so-called
{\em fixed LPs}),
implying that correctness can be formulated as constraints on the ordering of the
occurrences of linearization points of the methods in any program
execution.
\bjcom{Give list of works}
In our work, we use this technique to specify linearizability of
set implementations (see Section XXX).
%% a concept of data-independence, these constraints can be formulated as
%% a number of samll observers, which are automata that recognize violations
%% of these ordering requirements~\cite{AHHR:integrated}.

Unfortunately, in the TS Stack algorithm of Figure~\ref{ts-stack:fig},
LPs cannot be affixed to particular statements. For instance,  two overlapping
push operations may have to be linearized in different orders depending
on how their corresponding later pop operations are ordered in time.
%% not be determined during their invocation, but only later
%% when the ordering between the two corresponding pop operations is determined.
Fortunately, Bouajjanni etal.\ recently established that for stacks and
queues, linearizability can be precisely specified without refering to
linearization points~\cite{BEEH:icalp15}. They showed to linearizability
of concurrent stack implementations can be precisely described by
a small number of simple 
constraints on the ordering of call and return actions.
Let us describe how to represent and verify these constraints.

Let a call action on an operation $\mname(\data)$ be denoted
$\calltwo{\mname}{\data}$ and a return operation be denoted
$\returntwo{\mname}{\data}$. Ordering constraints on the sequence of
call and return actions for a small set of data values $\data$ can be
checked by an {\em observer},
as introduced in~\cite{AHHR:integrated}. 
%
Observers are
finite automata extended with a finite set of {\em registers}
that assume values in $\intgrs$. 
%
At initialization,
the registers are nondeterministically
assigned arbitrary values, which never change
during a run of the observer. 
%
%% Formally, an observer $\observer$ is a tuple
%% $\otuple$ where $\ostateset$ is a finite set 
%% of {\it observer states} including the 
%% {\it initial state} $\oinitstate$ and
%% the {\it accepting state} $\oaccstate$, 
%% a finite set $\ovarset$  
%% of {\it registers}, and $\otransitionset$ is a finite
%% set of {\it transitions}.
%% %
%% %
%% Transitions are of the form 
%% $\tuple{\ostate_1,\mname(\inxvar,\outxvar),\ostate_2}$ where 
%% $\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
Transitions are labeled by 
operations whose input or output data may be parameterized on registers.
%
%% The observer processes a trace one operation at a time.
%% %
%% If there is a transition, whose label, after replacing registers by their
%% values, matches the operation, such a transition is performed. 
%% %
%% If there is no
%% such transition, the observer remains in its current state.
%
The observer accepts a trace if it can  be processed in such a way that
an accepting state is reached.
%
The observer is defined in such a way that it accepts precisely those
traces that do {\em not} belong to the behavior
of the data structure.
As an example, Figure ~\ref{fig:nonpopobserver:fig} shows an observer which intuitively specifies that 
a data item must not be popped before it is popped. More precisely, it
specifies that the pop operation must not return before any push operation
for the same data value has been called.

\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(40pt,50pt)$) {$s_2$};



\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(80pt,0pt)$) {$s_1$};

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(80pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\footnotesize${\tt ret \; pop}(x)$} 
(s1);

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[left=5pt,pos=0.5] {\footnotesize${\tt call \; push}(x)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s0) to 
node[above=-15pt,pos=0.5] {\footnotesize${\tt call \; pop}(x)$} 
(s0);

\draw[->,>=stealth,line width=0.5pt, in=50,out=-30,loop] (s2) to 
node[right=2pt,pos=0.5] {\footnotesize${\tt act(x)}$} 
(s2);


%\node[]
%at ($(s2.center)+(60pt,20pt)$) {\footnotesize${\tt call \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,10pt)$) {\footnotesize${\tt ret \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,0pt)$) {\footnotesize${\tt call \; pop}(x)+$};
%\node[] at ($(s2.center)+(60pt,-10pt)$) {\footnotesize${\tt ret \; pop}(x)$};

\end{tikzpicture}
\caption{An observer recognizing differentiated traces which have a Pop with no corresponding Push. The variable $\tt x$ is an observer register, The action $\tt act$ ranges over all actions}
\label{fig:nonpopobserver:fig}
\end{figure}

%\todo[inline]{Quy: make a picture of this observer, and describe intuitively
%  how it works}


%\todo[inline]{The observer of Fig. 2 is not correct: You must also include
%  call push(x) which goes to a sink state. Moreover, you must explain that you
%  assume differentiated traces, Plese go over the observer again. Also mark
%  the initial location}

In~\cite{BEEH:icalp15}, is it shown that
a stack can precisely specified by four observers,
that specify the following intuitive constraints.
%\todo[inline]{Quy: Make sure that you have the right constraints. I just guessed that the below four are the right ones}
\begin{enumerate}
\item a data item must not be popped before it is pushed,
\item pop must not return ``empty'' if a data element was pushed but not
  popped
\item a pushed data item must not be popped twice
\item a data item must be popped with respect to last in first out stack order% \bjcom{Here we specify stack order}
\end{enumerate}
The exact form of observers for these constraints represent them in
terms of ordering of call an return actions. Observers for constraint
2 and 3 are similar to that for constraint 1. The observer for constraint
4 is more complex, and shown in Figure ~\ref{fig:lifostack:fig}. In the observer, $\tt ret \; pop(x_1)$ happens after sequences of actions $\tt call \; push(x_2)$, $\tt ret \; push(x_2)$, $\tt call \; pop(x_2)$ in which the pair $\tt ret \; push(x_2) / call \; pop(x_2)$ is after $\tt ret \; push(x_2)$ . The $\tt call \; push(x_1)/ret \;push(x_1)$ happens before all the sequence. The observer intuitively specifies that a data cannot be popped from the stack if there is always at least an different data above it in the Stack (regardless of how linearize the execution). 
%\todo[inline]{Please make a good explanation of Figure 3}
\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};


\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(60pt,0pt)$) {$s_1$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(120pt,0pt)$) {$s_2$};

\node(s3)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(180pt,0pt)$) {$s_3$};

\node(s4)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,0pt)$) {$s_4$};

\node(s6)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(300pt,0pt)$) {$s_6$};

\node(s5)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,-100pt)$) {$s_5$};

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(300pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; push}(x_1)$} 
(s1);
\draw[->,>=stealth,line width=0.5pt,] (s1) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_1)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,] (s2) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s3);

\draw[->,>=stealth,line width=0.5pt,] (s3) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; pop}(x_1)$} 
(s4);

\draw[->,>=stealth,line width=0.5pt,] (s4) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; pop}(x_1)$} 
(s6);

\draw[->,>=stealth,line width=0.5pt, in=50,out=130,loop] (s2) to 
node[above=2pt,pos=0.5] {\tiny${\tt call \; push}(x_2)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,out=-35,in=35] (s4) to 
node[right=-2pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s5);

\draw[->,>=stealth,line width=0.5pt,out=145,in=-145] (s5) to 
node[left=-2pt,pos=0.5] {\tiny${\tt call \; pop}(x_2)$} 
(s4);

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node(s7)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s5.center)+(-180pt,-2pt)$) {$s_7$};

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[above = -2pt,pos=0.5, sloped] {\tiny${\tt \neg{call \;push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s1) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg {ret \; push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s2) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; push(x_2) \wedge \neq{ret \; push(x_2)}}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s3) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; pop(x_1)}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s4) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{ret \; pop(x_1) \wedge \neg{ret \; pop(x_2)}}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s5) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; ret(x_2)}}$}  
(s7);

\end{tikzpicture}
\caption{An observer recognizing LIFO violations. The variables $\tt x_1$ and $\tt x_2$ are observer registers. For each action $\tt act$ associated with $\tt x_1$ and $\tt x_2$, $\tt \neg act$ ranges over all actions associated with $\tt x_1$ and $\tt x_2$ except for $\tt act$.  
}
\label{fig:lifostack:fig}
\end{figure}

\subsection{Verification by Fragment Abstraction}
To verify that TS-Stack is a correct linearizable implementation of a stack, we
must show that any concurrent execution of method calls satisfies the
above four constraints (1) -- (4).
To this end, define a {\em program} to consist 
of an arbitrary number of concurrently executing threads,
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
each of which executes a push or pop method with some data value as parameter.
We assume that the data structure has been initialized
by the {\tt init} method prior to the start of program execution.
We must verify that any execution of such a program satisfies the four
constraints above. To verify a property, 
we form, as in the automata-theoretic approach~\cite{VW:modelchecking},
also adopted in~\cite{AHHR:integrated},
the cross-product of the program  and the corresponding
observer, where the observer synchronizes with the program on call and
return actions that are recognized by the observer. This reduces the
problem of checking each of the above constraints to the problem of checking
that the observer cannot reach an accepting state.

\todo[inline]{We may have to say that we check the trivial conditions that
  were checked by the monitor in SAS 16}
%% Instrumentation to check conditions 
%% (i) - (iii) is added automatically, reducing the verification to a
%% reachability problem, i.e., to verify that no configuration that violates
%% conditions (i) - (iv) can be reached.

%% The addition of observers reduces the problem of checking linearizability to
%% checking control state reachability for the composition of an arbitrary
%% program and an observer.
%% \begin{enumerate}[(i)]
%% \item each method invocation generates a sequence of linearization events,
%%   of which only the last one may change the state of the observer,
%% \item
%%   the linearization event which is generated last
%%   is consistent with the call parameters and return value of the method,
%%   and
%% \item  that the sequence of linearization
%%   events cannot cause the observer to reach an accepting state.
%% \end{enumerate}
%% Tasks (i) and (ii) can be verified by standard instrumentations of
%% methods, which we will not further elaborate on here.
%% is added automatically, reducing the verification to a
%%  and our verification by standard techniques, as well as
%% the standard way, and verify condition (iv) by checking that the observer cannot
%% reach an accepting state.
In the actual verification, we compute a symbolic representation
of an invariant that is satisfied by all reachable program configurations (including observer states).
This invariant is obtained by an abstract-interpretation-based
fixpoint procedure, which starts
from a representation of the set of initial configurations, and
thereafter repeatedly performs
postcondition computations that extend the
symbolic representation by the effect of any execution step of the program,
until convergence.
This analysis needs to deal with the challenges of an unbounded domain of
data values, an unbounded number of concurrently executing threads, and an
unbounded heap. We have therefore developed a symbolic representation, which
is a careful combination of
data abstraction, thread abstraction, and shape abstraction. 
%% The analysis needs to deal with the challenges of an unbounded domain of
%% data values, an unbounded number of concurrently executing threads, and an
%% unbounded heap. We have therefore developed a technique for verification,
%% which uses a carefully designed combination of
%% data abstraction, thread abstraction, and shape abstraction.
Let us illustrate this symbolic representation on the algorithm in
Figure~\ref{ts-stack:fig}.

Our {\em data abstraction} abstracts from concrete data values in
$\intgrs$ by representing only {\em relations}
(in the set $\set{\prec,\equiv,\succ}$) between values of {\tt Data} fields of 
heap cells, local {\tt Data} variables, and registers of the observer.
We similarly abstract from concrete values of timestamps by representing
only relations between values of timestamps in heap cells and in
local variables.
In order to cope with an unbounded number of threads,
we extend the {\em thread-modular approach}~\cite{BLMRS:cav08}, in which
the symbolic representation characterizes
the projection of the configuration onto a single thread $t$.
%% The postcondition computation must consider how this projection can
%% be changed by execution steps of the program.
%% Such a step can be performed
%% either by the thread $t$ (a local step) or by 
%% some other thread $t'$ (an interference step).
%% Local steps can be handled by adapting standard techniques
%% for computing postconditions.
The corresponding postcondition computation must then take into account that
the projection onto a particular thread can be changed either by step of that
thread, or by a step of another thread.
\bjcom{omit this last sentence?}
%% interference steps involve to first compute a new abstract configuration
%% over two thread identifiers
%% which represents the combined state of both threads $t$ and $t'$,
%% and only therafter
%% performing the step of $t'$; thereafter the resulting abstract post-state
%% is projected back onto $t$.

A main contribution of this work is 
a novel {\em shape abstraction}, which is designed to cope with an unbounded
heap (note that the heap visible to a single thread is still unbounded).
The idea of this abstraction, called {\em fragment abstraction} is
to represent a heap by a set of (heap) {\em fragments}, where each fragment is
just a pair of nodes that are connected by a {\tt next} pointer. Thus, the
representation of a particular heap is obtained as the set of pairs of nodes
that are connected by a {\tt next pointer}. Conversely,
a set of (heap) fragments represents the set of heaps that can be formed by
putting together fragments from this set, where each fragment may be used
multiple times. In order to obtain a representation which is both expressive
and bounded, nodes are first abstracted to a finite set of node types, called
{\em tags}. A tag represents both local information about the values of
fields of the node
and about which pointer variables point to the node, as well as global
information about how the node is positioned in the overall projected heap.
The global information records from which global and thread-local pointer
variables the node can be reached by a sequence of pointers, as well as
which global variables can be reached from the node.
\bjcom{I believe we could omit this last sentence}

Let us illustrate our symbolic representation for the TS-Stack algorithm.
The heap in the algorithm consists of a set of singly linked lists (SLLs). Each
SLL is accessed from a particular pointer in an array of such pointers. 
Fig. ~\ref{fig:tsshape} shows an example of a snapshot of the heap state, in a situation when
% Quy write from here
it is accessed concurrently by three threads $\thread_1$, $\thread_2$, and $\thread_3$. The heap consists of three SLLs accessed from three pointer variables $\tt top_1$, $\tt top_2$, and $\tt top_3$ respectively. A variable that labels a node is the pointer variable which point to the node.
%\todo[inline]{Quy: Use a better scheme for indexing local variables. Previously,
%  we used {\tt var[2]}, for variable {\tt var} of thread 2. Maybe use it also here?}
%\todo[inline]{You must start by explaining the notation better. E.g., what means
%  a pointer variable that labels a node?}
The thread $\thread_1$ is trying to push a new node pointed by its local variable $\tt new[1]$ while $\thread_3$ has just called its push method and $\thread_2$ is doing its pop method. The thread $\thread_2$ find the youngest node in the list owned by $\thread_3$ which is determined by its local variable $\tt youngest[2]$.
%\todo[inline]{Quy: Make a picture of a concrete shape with several threads.
%  Say which operations are working there now, and give some
%  explanation of what the figure says}.
\begin{figure}
	\input tsshape
\caption{A concrete shape of timestamp stack with three threads}
\label{fig:tsshape}
\end{figure} 
%\quycom{I am trying to improve text from here}
%\todo[inline]{I do not quite understand that this is the result: Thread 2 is doing a pop from list 3: so it sees ${\tt top}_3$. But why does it see the
%  list 2? and why does it need to see the list 1? Does thread 2 really see all the nodes and variables in Figure 5?}
%As can be seen the the figure, the node pointed by $\tt new[1]$ is not visible with $\thread_3$ because it is still the private variable of $\thread_2$. We use three observer registers $\tt d_1$, $\tt d_2$, and $\tt d_3$. The data value $\tt 9$ is equal to $\tt d_1$, $\tt 8$ is equal to $\tt d_2$ and all other data values are equal to $\tt d_3$. To abstract timestamp of a node, we keep the ordering between its timestamp and nodes whose data values are equal to $\tt d_1$ or $\tt d_2$ which are described by $\tt to_1$,...,$\tt to_6$ in the figure.
% Quy write here
%\todo[inline]{Quy: make a picture of the correspondingq
%  thread view with data abstraction  Complete this description}.
%\begin{figure}
%	\input tsshapeb
%\caption{The projection on thread 2 after data abstraction}
%\label{fig:tsdabsshape}
%\end{figure} 

%In order to represent the projection in Fig. ~\ref{fig:tsdabsshape} by

\begin{figure}
	\input tsabsshape
\caption{Fragment abstraction}
\label{fig:tsviewshape}
\end{figure} 
To obtain our symbolic representation by a set of fragments, we first apply data abstraction and
project the configuration onto a single thread. The fig shows the result
of projecting onto thread 2, which performs a pop with data abstraction. Secondly, we identify the current working list and abstract the other lists where we do not distinguish the differences between variables in the other list. Finally, we define the information conveyed by node tags. We define one tag for push operations and one for pop operations, since
their methods use different sets of local variables.
For a pop operation, a {\em tag} is defined as a tuple
$\tuple{\pvars,\data,\reachfrom,\reachto,\private}$, where
\begin{itemize}
\item
  $\pvars$ is a set of pointer variables (i.e., a subset of
  {\tt top, ctop, youngest, n, myTop}
  %\bjcom{Quy: Check exactly what this set should be}
\item
  $\vals$ maps
  \begin{inparaenum}[(i)]
  \item
    each {\tt mark} field to a boolean value,
  \item
    each {\tt d} field to a mapping from observer registers to $\set{=, \neq}$,
  \item
    each {\tt time} field to a mapping from each observer register to a subset of $\set{<,=,>}$ %\bjinsert{Quy: insert the correct text here}.
  \end{inparaenum}
\item
  $\reachfrom$ and $\reachto$ are sets of global variables and observer registers, %\bjcom{Quy: Check what is really needed here: Maybe we need only reachfrom?}
\item
  $\private$ is a boolean value.
\end{itemize}
A node $\cell$ in the heap of a thread view satisfies a tag $\tagtuple$,
denoted $\cell \lhd \tagtuple$ if
\begin{itemize}
\item
  $\pvars$ is the set of pointer variables that point to $\cell$.
  %\bjcom{Quy: Or Should it be contains?}
\item
The value of {\tt mark} field of $\cell$ is equal to the value of $\vals\tt {.(mark)}$. For each observer register $\tt d_i$, the relation of {\tt d} field  and $\tt d_i$ is a subset of $\vals\tt {.(d).(d_i)}$, and the relation between value of {\tt time} field and cells whose $\tt d$ field is equal to $\tt d_i$ is a subset of $\vals\tt {.(time).(d_i)}$. 
  %\bjcom{Quy: continue to write here}
\item $\reachfrom$ and $\reachto$ are sets of global variables and observer registers in which the cell is reachable and reach to.
\item $\private$ is the private information of the cell.
\end{itemize}

We can now define the concept of fragment.
A {\em fragment} is a pair $\tuple{\inputtag,\outputtag}$ of tags.
A {\em symbolic configuration} is a pair consisting of a set a value of a program
pointer, observer state, and value of local variables \bjcom{or: local state}, and a set of fragments.
%\todo[inline]{Quy: Could you explain exactly what is in a symbolic configuration?
%  Does it contain some information about values of local variables, such as
%  {\tt success} and {\tt maxTS}?}

%\todo[inline]{Quy: make a picture with all the fragments obtained from the
 % thread projection}
The Fig. ~\ref{fig:tsviewshape} shows the symbolic representation of projection on to thread 2 from Fig. ~\ref{fig:tsshape}. Under each node, the $\reachfrom$ of the node is shown in the first row, whereas the $\reachto$ is shown in the second row. The fragments $\tt v_1$, $\tt v_2$, $\tt v_3$, and $\tt v_4$ are obtained from the other lists. Whereas, $\tt v_5$, $\tt v_6$, $\tt v_7$, and $\tt v_8$ are obtained from the current list.   
A thread-abstracted configuration satisfies a symbolic configuration if
\begin{itemize}
\item it satisfies program counter, observer state, and local variables %\bjcom{Say that program counter, and local variables ...}
\item
  For each cell $\cell$ in the heap of the configuraion, there is
  a fragment $\tuple{\inputtag,\outputtag}$ in the set of fragments, such
  that $\cell \lhd \inputtag$ and ${\tt next}(\cell) \lhd \outputtag$.
  \bjcom{check the case when $\cell$ points to null etc.}
\end{itemize}

symbolic configurations. 
A {\em symbolic representation} $\symbrep$ is a set of
symbolic configurations. 
A  set of thread-abstracted configurations satisfies a symbolic
representation $\symbrep$ if each of its thread-abstracted
configuration satisfies some symbolic configuration in $\symbrep$.





