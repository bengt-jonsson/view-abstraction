\section{Overview}
\label{sec:overview}

In this section, we illustrate our technique by using it to prove correctness, in
the sense of linearizability, of
a concurrent data structure implementation, namely the Timestamped Stack
of~\cite{ts-stack}. Correctness of this algorithm has previously not been
verified by automated techniques.

\subsection{The Algorithm}

\input tscode

%\todo[inline]{Provide a description of the algorithm}
%\todo[inline]{Quy: The line numbers for Pop are scrambled. Can you fix?
%Also, the variable {\tt i} at line 6 is initialized strangely. Can you fix?}
Figure~\ref{ts-stack:fig} shows a simplified version of the Timestamped Stack (TS stack), where we have omitted the check for emptiness in the ${\tt pop}$ method, and the optimization using {\tt push}-{\tt pop} elimination. These features are included in the full version of the algorithm, described in Appendix XXX, that we have verified automatically.

The algorithm uses an array of singly-linked lists (SLLs), one for each thread, accessed via the thread-indexed array ${\tt pools[maxThreads]}$ of pointers to the first cell of each list. The ${\tt init}$ method initializes each of these pointers to $\nullconst$. Each list cell contains a data value, a timestamp value, a ${\tt next}$ pointer, and a boolean flag ${\tt mark}$ which indicates whether the node is logically removed from the stack. Each thread pushes elements to its own list, and can pop elements from any list.

A ${\tt push}$ method for inserting an element ${\tt d}$ works as follows: first, a new cell with element ${\tt d}$ and minimal timestamp ${\tt -1}$ is inserted at the beginning of the list which is owned by the calling thread (line 1-3). After that, a new timestamp is created and assigned (via the variable ${\tt t}$) to the ${\tt ts}$ field of the inserted cell (line 4-5).
%\bjcom{Quesion to Quy: What happens when pop methods sees uninitialized timestamps?} \quycom{uninitialized time is -1, its smaller than any other time.}
Finally, the method unlinks (i.e., physically removes) all cells, reachable from the inserted cell, whose ${\tt mark}$ field is ${\tt true}$; these cells are already logically removed. This is done by redirecting the $\tt next$ pointer of the inserted cell to the first cell with a $\false$ $\tt mark$ field, which is
reachable from the inserted cell through a sequence of $\tt next$ pointers.

A ${\tt pop}$ method first traverses all the lists, finding
the first element whose ${\tt mark}$ field is ${\tt false}$ in each list (line 8), and letting the variable ${\tt youngest}$ point to the most recent such cell
(i.e., with the largest timestamp) (line 1-11).
A compare-and-swap (CAS) is used
to set the ${\tt mark}$ field of this youngest cell to $\true$,
thereby logically removing it.
This procedure will restart if the CAS fails. After the youngest cell has been removed, the method will unlink all cells, whose ${\tt mark}$ field is ${\tt true}$,
that appear before (line 17-19) or after (line 20-23) the removed cell.
Finally, the method returns the ${\tt data}$ value of the removed cell.
%\todo[inline]{Comments by Bengt: How can you return empty? Also, I think you
 % need to explain the extra removal of marked cells better} \quycom{this code is a simplified version without emptiness checking. If we add emptiness part, the code will be longer and more complicated}

\subsection{Specifying the Correctness Criterion of Linearizability}
In our verification, we shall establish that the TS stack algorithm of
Figure~\ref{ts-stack:fig} is is correct in the sense that it is a
linearizable implementation of a stack data structure.
Linearizability intuitively states that
each operation on the data structure can be considered as being
performed atomically at some point, called the {\em linearization point (LP)},
between its invocation and return~\cite{HeWi:linearizability}.
For many data structure implementations, LPs can be statically
affixed to particular statements in method implementations,
%% (so-called {\em fixed LPs}),
implying that correctness can be formulated as constraints on the ordering of the
occurrences of LPs in any program execution.
\bjcom{Give list of works?}
In our work, we use this technique to specify linearizability of
set implementations (see Section XXX).
%% a concept of data-independence, these constraints can be formulated as
%% a number of samll observers, which are automata that recognize violations
%% of these ordering requirements~\cite{AHHR:integrated}.
Unfortunately, this technique cannot be applied to the TS stack algorithm.
%% does not LPs cannot be affixed to particular statements. For instance,  two overlapping
%% push operations may have to be linearized in different orders depending
%% on how their corresponding later pop operations are ordered in time.
%% not be determined during their invocation, but only later
%% when the ordering between the two corresponding pop operations is determined.
Instead, we use a recent technique of~\cite{BEEH:icalp15,HSV:concur13}, who show
that for stacks and queues,
linearizability can be precisely specified by a small number of
constraints on the ordering of call and return actions of the methods,
without any reference to LPs.
In the case of a stack, these constraints intuitively express that
%\todo[inline]{Quy: Make sure that you have the right constraints. I just guessed that the below four are the right ones}
\begin{enumerate}
\item a data item must not be popped before it is pushed,
\item pop must not return ``empty'' if some data element was pushed but not
  popped,
\item a pushed data item must not be popped twice, and
\item any two data items must be popped in last in first out stack order.% \bjcom{Here we specify stack order}
\end{enumerate}
These constraints must be precisely expressed in terms of allowed
sequences of calls
and returns of different methods. \cite{BEEH:icalp15} show how this can be
done using observers, introduced in~\cite{AHHR:integrated}. 
We now show how this can be done for a stack implementation.

Let $\calltwo{\tt push}{\tt d}$ and $\returntwo{\tt push}{\tt d}$ denote the
action of calling or returning from a ${\tt push}$ method with data parameter ${\tt d}$.
Let $\calltwo{\tt pop}{\tt d}$ and $\returntwo{\tt pop}{\tt d}$ denote the
action of calling or returning from a ${\tt pop}$ method which returns
the value ${\tt d}$.
%% Let a call action on an operation $\mname(\data)$ be denoted
%% $\calltwo{\mname}{\data}$ and a return action be denoted
%% $\returntwo{\mname}{\data}$.
Constraints on the ordering of such actions can be
checked by an {\em observer}.
%
Observers are
finite automata extended with a finite set of {\em registers} whose values range
over the domain of data values.
%
At initialization,
the registers are nondeterministically
assigned arbitrary distinct data values, which never change
during a run of the observer. 
%
%% Formally, an observer $\observer$ is a tuple
%% $\otuple$ where $\ostateset$ is a finite set 
%% of {\it observer states} including the 
%% {\it initial state} $\oinitstate$ and
%% the {\it accepting state} $\oaccstate$, 
%% a finite set $\ovarset$  
%% of {\it registers}, and $\otransitionset$ is a finite
%% set of {\it transitions}.
%% %
%% %
%% Transitions are of the form 
%% $\tuple{\ostate_1,\mname(\inxvar,\outxvar),\ostate_2}$ where 
%% $\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
Transitions are labeled by actions whose data value
is the value of a register parameterized on registers.%
The observer processes a sequence of actions, one action at a time.
If there is a transition, whose label, after replacing registers by their
values, matches the action, such a transition is performed; otherwise
the observer remains in its current state.
%% %
%% If there is no
%% such transition, 
%
The observer accepts a sequence of call and return actions
if, for some initial assignment of data values to its registers, the sequence
can  be processed in such a way that an accepting state is reached.
The observer for a particular constraint is defined in such a way that it
accepts precisely those sequences  that do {\em not} satisfy the constraint.


\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(40pt,50pt)$) {$s_2$};



\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(80pt,0pt)$) {$s_1$};

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(80pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\footnotesize${\tt ret \; pop}(x)$} 
(s1);

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[left=5pt,pos=0.5] {\footnotesize${\tt call \; push}(x)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s0) to 
node[above=-15pt,pos=0.5] {\footnotesize${\tt call \; pop}(x)$} 
(s0);

%% \draw[->,>=stealth,line width=0.5pt, in=50,out=-30,loop] (s2) to 
%% node[right=2pt,pos=0.5] {\footnotesize${\tt act(x)}$} 
%% (s2);


%\node[]
%at ($(s2.center)+(60pt,20pt)$) {\footnotesize${\tt call \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,10pt)$) {\footnotesize${\tt ret \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,0pt)$) {\footnotesize${\tt call \; pop}(x)+$};
%\node[] at ($(s2.center)+(60pt,-10pt)$) {\footnotesize${\tt ret \; pop}(x)$};

\end{tikzpicture}
\caption{An observer recognizing sequences in which ${\tt pop}$ for some data value returns although no
  corresponding ${\tt push}$ has been previously called. The observer has one register $\tt x$.}
\label{fig:nonpopobserver:fig}
\end{figure}

%\todo[inline]{Quy: make a picture of this observer, and describe intuitively
%  how it works}


%\todo[inline]{The observer of Fig. 2 is not correct: You must also include
%  call push(x) which goes to a sink state. Moreover, you must explain that you
%  assume differentiated traces, Plese go over the observer again. Also mark
%  the initial location}

As an example, Figure~\ref{fig:nonpopobserver:fig} shows an observer which
makes constraint 1 precise, by specifying
that a ${\tt pop}$ method must not return before any ${\tt push}$ method
with the same data value has been called.
%% The exact form of observers for these constraints represent them in
%% terms of ordering of call an return actions.
Observers for constraints 2 and 3 are similar to that for constraint 1.
The observer for constraint 4 is more complex,
and shown in Figure~\ref{fig:lifostack:fig}.
It has two registers $\tt x_1$ and $\tt x_2$. Let $\tt d_1$ and $\tt d_2$
denote their initial values.
In a sequence of actions that leads to the accepting state $s_6$, a
${\tt push(d_1)}$ must be linearized before a ${\tt push(d_2)}$, since
${\tt push(d_1)}$ returns before ${\tt push(d_2)}$ is called; however
a ${\tt pop(d_1)}$ returns in a situation where the number of returned
${\tt push(d_2)}$ methods is larger than the number of called
${\tt pop(d_2)}$ methods, thereby thereby violating LIFO ordering.
%% ${\tt pop(d_2)}$ which leaves some data value $\tt d_2$ on top of
%% $\tt d_1$ in the stack, also violating LIFO ordering.
%% In the observer, the action which leads to an accepting state $\tt ret \; pop(x_1)$ happens after sequences of actions namely $\tt call \; push(x_2)$, $\tt ret \; push(x_2)$, $\tt call \; pop(x_2)$ in which the pair $\tt ret \; push(x_2) / call \; pop(x_2)$ is always after $\tt ret \; push(x_2)$ . The $\tt call \; push(x_1)/ret \;push(x_1)$ happens before all the sequences. Intuitively, the observer specifies that a data cannot be popped from the stack if there is always at least an different data above it in the Stack (regardless of how linearize the execution). 
%\todo[inline]{The explanation of Figure 3 to be improved}
\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};


\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(60pt,0pt)$) {$s_1$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(120pt,0pt)$) {$s_2$};

\node(s3)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(180pt,0pt)$) {$s_3$};

\node(s4)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,0pt)$) {$s_4$};

\node(s6)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(300pt,0pt)$) {$s_6$};

\node(s5)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,-100pt)$) {$s_5$};

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(300pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; push}(x_1)$} 
(s1);
\draw[->,>=stealth,line width=0.5pt,] (s1) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_1)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,] (s2) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s3);

\draw[->,>=stealth,line width=0.5pt,] (s3) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; pop}(x_1)$} 
(s4);

\draw[->,>=stealth,line width=0.5pt,] (s4) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; pop}(x_1)$} 
(s6);

\draw[->,>=stealth,line width=0.5pt, in=50,out=130,loop] (s2) to 
node[above=2pt,pos=0.5] {\tiny${\tt call \; push}(x_2)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,out=-35,in=35] (s4) to 
node[right=-2pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s5);

\draw[->,>=stealth,line width=0.5pt,out=145,in=-145] (s5) to 
node[left=-2pt,pos=0.5] {\tiny${\tt call \; pop}(x_2)$} 
(s4);

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node(s7)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s5.center)+(-180pt,-2pt)$) {$s_7$};

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[above = -2pt,pos=0.5, sloped] {\tiny${\tt \neg{call \;push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s1) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg {ret \; push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s2) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; push(x_2) \wedge \neq{ret \; push(x_2)}}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s3) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; pop(x_1)}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s4) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{ret \; pop(x_1) \wedge \neg{ret \; pop(x_2)}}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s5) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; ret(x_2)}}$}  
(s7);

\end{tikzpicture}
\caption{An observer recognizing LIFO violations. The registers are $\tt x_1$ and $\tt x_2$. For each action $\tt act$ parameterized by $\tt x_1$ or $\tt x_2$, the label $\tt \neg act(x_i)$ is the union of  all actions parameterized by $\tt x_1$ or $\tt x_2$ except for $\tt act(x_i)$.  
}
\label{fig:lifostack:fig}
\end{figure}

To verify that TS Stack is a correct linearizable implementation of a stack, we
must show that any concurrent execution of method calls satisfies the
above four constraints (1) -- (4).
To this end, define a {\em program} to consist 
of an arbitrary number of concurrently executing threads,
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
each of which executes a push or pop method with some data value as parameter.
We assume that the data structure has been initialized
by the ${\tt init}$ method prior to the start of program execution.
We must verify that any execution of such a program satisfies the four
constraints above. To verify a constraint, 
we form, as in the automata-theoretic approach~\cite{VW:modelchecking},
also adopted in~\cite{AHHR:integrated},
the cross-product of the program  and the corresponding
observer, where the observer synchronizes with the program on call and
return actions that are recognized by the observer. This reduces the
problem of checking each of the above constraints to the problem of checking
that the corresponding observer cannot reach an accepting state.

\todo[inline]{We may have to say that we check the trivial conditions that
  were checked by the monitor in SAS 16}
%% Instrumentation to check conditions 
%% (i) - (iii) is added automatically, reducing the verification to a
%% reachability problem, i.e., to verify that no configuration that violates
%% conditions (i) - (iv) can be reached.

%% The addition of observers reduces the problem of checking linearizability to
%% checking control state reachability for the composition of an arbitrary
%% program and an observer.
%% \begin{enumerate}[(i)]
%% \item each method invocation generates a sequence of linearization events,
%%   of which only the last one may change the state of the observer,
%% \item
%%   the linearization event which is generated last
%%   is consistent with the call parameters and return value of the method,
%%   and
%% \item  that the sequence of linearization
%%   events cannot cause the observer to reach an accepting state.
%% \end{enumerate}
%% Tasks (i) and (ii) can be verified by standard instrumentations of
%% methods, which we will not further elaborate on here.
%% is added automatically, reducing the verification to a
%%  and our verification by standard techniques, as well as
%% the standard way, and verify condition (iv) by checking that the observer cannot
%% reach an accepting state.

\subsection{Verification by Fragment Abstraction}

In the actual verification, we must compute a symbolic representation
of an invariant that is satisfied by all reachable configurations of
the cross-product of the program  and an observer.
The verification must address the challenges of an unbounded domain of
data values, an unbounded number of concurrently executing threads, and an
unbounded heap.
For this, we have developed a novel shape representation, called
{\em fragment abstraction}, which can also be combined with
data abstraction and thread abstraction.
%% This invariant is obtained by an abstract-interpretation-based
%% fixpoint procedure, which starts
%% from a representation of the set of initial configurations, and
%% thereafter repeatedly performs
%% postcondition computations that extend the
%% symbolic representation by the effect of any execution step of the program,
%% until convergence.
%% The analysis needs to deal with the challenges of an unbounded domain of
%% data values, an unbounded number of concurrently executing threads, and an
%% unbounded heap. We have therefore developed a technique for verification,
%% which uses a carefully designed combination of
%% data abstraction, thread abstraction, and shape abstraction.
%% Let us illustrate this symbolic representation on the algorithm in
%% Figure~\ref{ts-stack:fig}.
\begin{itemize}
  \item
    Our {\em data abstraction} abstracts from actual values variables and
    fields of heap cells that represent data (they range over $\tt int$
    in Figure~\ref{ts-stack:fig}),
%%     of $\intgrs$-valued variables and $\intgrs$-fields of heap cells,
    into possible relations
    (in the set $\set{<,=,>}$) with global data variables and values of
    observer registers. 
    In order to verify that the observer cannot reach an accepting state,
    it is necessary to
    keep track of heap cells whose ${\tt data}$ field has the same value as an
    observer register. Thus, 
    for an observer register $\reg_i$, let $\cell$ be a {\em $\reg_i$-cell} if
    the ${\tt data}$ field of $\cell$ has the same value as $\reg_i$.
    Then, for timestamps, our data abstraction represents
    possible relations with timestamps of $\reg_i$- cells.
%% between values of ${\tt Data}$ fields of 
%% heap cells, local ${\tt Data}$ variables, and registers of the observer.
%% We similarly abstract from concrete values of timestamps by representing
%% only relations between values of timestamps in heap cells and in
%% local variables.
  \item
We cope with an unbounded number of threads by
adapting the {\em thread-modular approach}~\cite{BLMRS:cav08},
representing only the part of the global configuration that is visible to
a single thread $\thread$; this part includes the local state of $\thread$ and
the part of the heap that $\thread$ can access through its local
and global pointer variables.
\item
  We cope with an unbounded heap by a novel shape abstraction, 
called {\em fragment abstraction}. This abstraction 
represents a heap by a set of (heap) {\em fragments}, where each fragment is
a pair of node types (called {\em tags}) that are connected by a pointer.
A tag is an abstraction of a heap cell, which contains both 
\begin{inparaenum}[(i)]
\item
  local information about its non-pointer fields, obtained
   using our data abstraction,
 \item
   global information, saying how
it can be reached from and reach to (by following a chain of pointers)
other heap cells that are pointed to by global variables, or cells whose
    data field has  the same value as some observer register.
\end{inparaenum}
A set of fragments represents the set of heap structures in which each
pair of pointer-connected cells are represented by some fragment in the set.
By construction, our fragment abstraction is finitary, since there is
a bounded set of tags.
\end{itemize}
%% just a pair of cells that are connected by a ${\tt next}$ pointer. Thus, the
%% representation of a particular heap is obtained as the set of pairs of cells
%% that are connected by a ${\tt next pointer}$. Conversely,
%% a set of (heap) fragments represents the set of heaps that can be formed by
%% putting together fragments from this set, where each fragment may be used
%% multiple times. In order to obtain a representation which is both expressive
%% and bounded, cells are first abstracted to a finite set of cell types, called
%% {\em tags}. A tag represents both local information about the values of
%% fields of the cell
%% and about which pointer variables point to the cell, as well as global
%% information about how the cell is positioned in the overall projected heap.
%% The global information records from which global and thread-local pointer
%% variables the cell can be reached by a sequence of pointers, as well as
%% which global variables can be reached from the cell.
%% The postcondition computation must consider how this projection can
%% be changed by execution steps of the program.
%% Such a step can be performed
%% either by the thread $t$ (a local step) or by 
%% some other thread $t'$ (an interference step).
%% Local steps can be handled by adapting standard techniques
%% for computing postconditions.
%% The corresponding postcondition computation must then take into account that
%% the projection onto a particular thread can be changed either by step of that
%% thread, or by a step of another thread.
%% interference steps involve to first compute a new abstract configuration
%% over two thread identifiers
%% which represents the combined state of both threads $t$ and $t'$,
%% and only therafter
%% performing the step of $t'$; thereafter the resulting abstract post-state
%% is projected back onto $t$.
Let us illustrate our fragment abstraction on the TS stack algorithm.
Its heap consists of a set of singly linked lists (SLLs), each of which
is accessed from a pointer in the array ${\tt pools[maxThreads]}$.
Figure~\ref{fig:tsshape} shows an example heap state,
in a configuration when % Quy write from here
it is accessed concurrently by three threads $\thread_1$, $\thread_2$, and $\thread_3$. The heap consists of three SLLs accessed from the three pointers $\tt pools[1]$, $\tt pools[2]$, and $\tt pools[3]$ respectively. Each heap cell is
shown with the values of its fields, using the layout shown to the right in
Figure~\ref{fig:tsshape}.
In addition, each cell is labeled by the
pointer variables that point to it. We use ${\tt lvar[i]}$ to denote the local
variable ${\tt lvar}$ of thread $\thread_i$.
%\todo[inline]{Quy: Use a better scheme for indexing local variables. Previously,
%  we used ${\tt var[2]}$, for variable ${\tt var}$ of thread 2. Maybe use it also here?}
%\todo[inline]{You must start by explaining the notation better. E.g., what means
%  a pointer variable that labels a node?}

Thread $\thread_1$ is trying to push a new node with data value $4$, pointed by its local variable $\tt new$, having reached line 3.
Thread $\thread_3$ has just called the ${\tt push}$ method.
Thread $\thread_2$ has reached line 12 in the execution of the ${\tt pop}$ method,  and has just assigned ${\tt youngest}$ to the first node in the list
pointed to by $\tt pools[3]$ which is not logically removed (in this case it is the last node of that list).
The local state of thread $\thread_2$
%% which executes the ${\tt pop}$ method
consists of the values of its program counter, the boolean variable ${\tt success}$, the pointer variables ${\tt youngest}$, ${\tt myTop}$, and ${\tt n}$,
and the timestamp variable ${\tt maxTS}$.
In verification, we must also consider the observer. In this case, the added
observer is the one in Figure~\ref{fig:lifostack:fig}, which has two registers
$\tt x_1$ and $\tt x_2$, which are assigned the values $9$ and $16$,
respectively.
\todo[inline]{to Quy: what about the variable ${\tt k}$?}

\begin{figure}
	\input tsshape
\caption{A possible heap configuration of TS stack with three threads.}
\label{fig:tsshape}
\end{figure} 
%\quycom{I am trying to improve text from here}
%\todo[inline]{I do not quite understand that this is the result: Thread 2 is doing a pop from list 3: so it sees ${\tt top}_3$. But why does it see the
%  list 2? and why does it need to see the list 1? Does thread 2 really see all the nodes and variables in Figure 5?}
%As can be seen the the figure, the node pointed by $\tt new[1]$ is not visible with $\thread_3$ because it is still the private variable of $\thread_2$. We use three observer registers $\tt d_1$, $\tt d_2$, and $\tt d_3$. The data value $\tt 9$ is equal to $\tt d_1$, $\tt 8$ is equal to $\tt d_2$ and all other data values are equal to $\tt d_3$. To abstract timestamp of a node, we keep the ordering between its timestamp and nodes whose data values are equal to $\tt d_1$ or $\tt d_2$ which are described by $\tt to_1$,...,$\tt to_6$ in the figure.
% Quy write here
%\todo[inline]{Quy: make a picture of the correspondingq
%  thread view with data abstraction  Complete this description}.
%\begin{figure}
%	\input tsshapeb
%\caption{The projection on thread 2 after data abstraction}
%\label{fig:tsdabsshape}
%\end{figure} 

%In order to represent the projection in Fig. ~\ref{fig:tsdabsshape} by

Our symbolic representation represents the reachable local states of a thread by
a finite set of {\em local symbolic configurations}, using our
data abstraction.
It maps each local symbolic configuration to a set of fragments, which
is a symbolic representation of the set of corresponding reachable heaps.
Thus, our symbolic representation is a mapping from a set of local symbolic
configurations, which maps each local symbolic configuration in its domain
to a set of fragments.
A global configuration satisfies a symbolic representation $\symbrep$
if the local stat of each thread $\thread$ satisfies some local symbolic
configuration in the domain of $\symbrep$, which is mapped to a set
of fragments, which represents the heap that is accessible to $\thread$ in
the global configuration. In the following, we explain more precisely the
local symbolic configurations, and our fragment abstraction.

\begin{figure}
	\input tsabsshape
\caption{Fragment abstraction}
\label{fig:tsviewshape}
\end{figure} 
A local configuration of a thread which executes the ${\tt pop}$ method consists
of the values of the program counter, the boolean variable ${\tt success}$, the
pointer variables ${\tt youngest}$, ${\tt myTop}$, ${\tt n}$,
and the timestamp variable ${\tt maxTS}$.
A {\em local symbolic configuration} is defined by applying our data abstraction
to the local non-pointer variables.  For a ${\tt pop}$ method, it maps
the program counter and ${\tt success}$ to their current values. It also maps
${\tt maxTS}$ to a mapping from each observer register $\reg_i$
to $\set{<,=,>}$, such that if $t$ is the value of some $\reg_i$-node's
$\tt ts$-field, then ${\tt maxTS} \sim t$ for some $\sim$ in this set.

In the configuration of Figure~\ref{fig:tsshape}, thread $\thread_2$
satisfies the local symbolic configuration shown in the green towards the
bottom, 
which maps the program counter to line 12, ${\tt success}$ to ${\tt false}$ and ${\tt maxTS}$ to the mapping from observer registers which maps
$\tt x_1$ to $\set{>}$ and $\tt x_2$ to $\set{=}$
(recall that the values of $\tt x_1$ and $\tt x_2$ are $9$, $16$ respectively).

\todo[inline]{To Quy: what is the value of ${\tt MaxTS}$? There is no
  node with data value $16$, so how can this be?}

%% Secondly, we identify the current working list and abstract the other lists where we do not distinguish the differences between variables in the other list.

Let us now show how our fragment abstraction represents the heap that is
accessible to $\thread_2$.
%% We must first define the notion of tag.
%% Finally, we define the information conveyed by node tags. We define one tag for push operations and one for pop operations, since
%% their methods use different sets of local variables.
The local pointer variables of $\thread_2$ are 
${\tt youngest}$, ${\tt myTop}$, and ${\tt n}$. Its global variables are of form
${\tt pools[i]}$, where ${\tt i}$ is a thread index, which we abstract into the
two variables ${\tt pools[me]}$, representing ${\tt pools[3]}$
which points to the list which $\thread_2$ is currently accessing,
and ${\tt pools[other]}$ representing ${\tt pools[i]}$ for $\tt i \neq 3$.
%\todo[inline]{Quy: here, we need a good way to explain how to represent ${\tt pools[i]}$
%  and ${\tt pools[k]}$}
%\todo[inline]{Thus, which are the global variables?}
%% For a thread which executes a $\tt pop$ operation, 
Define a {\em tag} as a tuple
$\tuple{\pvars,\vals,\reachfrom,\reachto,\private}$, where
\begin{itemize}
\item
  $\pvars$ is a set of local and global pointer variables
%%   of the ${\tt pop}$ method and (i.e., a subset of ${\tt top, ctop, youngest, n, myTop}$
  %\bjcom{Quy: Check exactly what this set should be}
\item
  $\vals$ is an abstraction of the non-pointer fields of a cell,
  i.e., it maps
  \begin{inparaenum}[(i)]
  \item
    the ${\tt mark}$ field to a boolean value,
  \item
    the ${\tt data}$ field to a mapping from observer registers to $\set{=, \neq}$,
  \item
    the ${\tt ts}$ field to a mapping from observer registers to subsets of $\set{<,=,>}$.
  \end{inparaenum}
\item
  $\reachfrom$ and $\reachto$ are sets of global variables and observer registers, and %\bjcom{Quy: Check what is really needed here: Maybe we need only reachfrom?}
\item
  $\private$ is a boolean value.
\end{itemize}
%% For an observer register $\reg_i$, let $\cell$ be a {\em $\reg_i$-cell} if
%% the ${\tt data}$ field of $\cell$ has the same value as $\reg_i$.
A heap cell $\cell$ which is accessible to $\thread_2$ satisfies a tag
with respect to $\thread_2$,
denoted $\cell \satfrag{{\thread_2}} \tagtuple$ if
\begin{itemize}
\item
  $\pvars$ is the set of global and local (to $\thread_2$)
  pointer variables that point to $\cell$,
  %\bjcom{Quy: Or Should it be contains?}
\item
the non-pointer fields of $\cell$ satisfies $\vals$ under our data abstraction, i.e.
  \begin{inparaenum}[(i)]
  \item
    $\vals({\tt mark})$ is the value of its ${\tt mark}$ field,
  \item
    for each observer register $\reg_i$, if $\cell$ is (is not) a
    $\reg_i$-cell, then $= \in \vals({\tt data})$ ($\neq \in \vals({\tt data})$),
  \item
    let $ts$ be the value of its ${\tt ts}$ field, then
    for each observer register $\reg_i$, if
    the value $t$ of the ${\tt ts}$ field of some $\reg_i$-cell
    satisfies $ts \sim t$, then $\sim \in \vals({\tt ts})$,
  \end{inparaenum}
\item $\reachfrom$ is the set of global variables from which $\cell$ is 
  reachable (by a chain of ${\tt next}$) pointers), and the set of
  observer registers $\reg_i$ such that $\cell$ is reachable from some $\reg_i$-cell,
\item $\reachto$ is the set of global variables that point to a cell that
  is reachable from $\cell$ and  the set of
  observer registers $\reg_i$ such that some $\reg_i$-cell is reachable from $\cell$,
\item $\private$ is $\true$ only if $\cell$ cannot be accessed from any
  other thread than $\thread_2$.
\end{itemize}

We can now define the concept of fragment.
A {\em fragment} $\frag$ is a pair of form $\tuple{\inputtag,\outputtag}$, 
of form $\tuple{\inputtag,\nullconst}$, or 
of form $\tuple{\inputtag,\dangconst}$.
We write  
$\cell \satfrag{{\thread_2}} \tuple{\inputtag,\outputtag}$ if
the ${\tt next}$ field of $\cell$ points to a cell $\cell'$ such that
$\cell \satfrag{{\thread_2}} \inputtag$ and
$\cell' \satfrag{{\thread_2}} \outputtag$.
For fragments of form $\tuple{\inputtag,\nullconst}$ 
or $\tuple{\inputtag,\dangconst}$, the definition is adapted in the obvious way.
A global configuration satisfies a set of fragments wrp.\ to $\thread_2$ if
for any cell $\cell$ that is accessible to $\thread_2$, it contains a fragment
$\frag$ such that $\cell \satfrag{{\thread_2}} \frag$.

%\todo[inline]{Quy: can you revise from here, and explain carefully the
%  local symbolic configuration, and set of fragments in 
%  Figure~\ref{fig:tsviewshape}
%}

%\todo[inline]{to Quy: Comments on Figure~\ref{fig:tsviewshape}:
%  I suggest to let the data field be a SET, use []- notation for the mappings of
%  the $\lambda_i$, you had some nice notation before for reachfrom and
%  reachto}
%\todo[inline]{Quy: make a picture with all the fragments obtained from the
 % thread projection}
Figure~\ref{fig:tsviewshape} shows a set of fragments that is
satisfied wrp.\ to $\thread_2$ by configuration in Figure~\ref{fig:tsshape}.
Each tag in a fragment is depicted with the same layout as heap cells.
The non-pointer fields are replaced by their corresponding data abstractions. 
For the $\tt data$ field, a register $\reg_i$ denotes that $\reg_i$ is mapped to
$\set{=}$, and a $\circ$ that both registers are mapped to $\set{\neq}$.
The ${\tt ts}$ field has can be mapped to four different mappings: $\lambda_1$, $\lambda_2$, $\lambda_3$ and $\lambda_4$, which are shown in the
yellow box at the bottom.
%% which are the mappings from observers registers to subsets of $\set{<,=,>}$ as described in the figure.
The set $\pvars$ is shown above the tag.
The sets $\reachfrom$ and $\reachto$ are shown below each tag, with
$\reachfrom$ is in the first row and $\reachto$ in the second row.

The fragments $\tt v_1$, $\tt v_2$, $\tt v_3$, and $\tt v_4$ represent cells
in the first two lists, whereas, $\tt v_5$, $\tt v_6$, $\tt v_7$, and $\tt v_8$
represent cells in the list pointed to by $\tt pools[3]$
%% The data field of a cell is presented by $\tt x_1$ if its value is equal to $\tt x_1$, $\tt x_2$ if its value is equal to $\tt x_2$ and $\circ$ if its value is different from both $\tt x_1$ and $\tt x_2$. 


%% The local symbolic configuration maps the program counter to the value 12, ${\tt success}$ to ${\tt false}$ and ${\tt maxTS}$ to a mapping from observer registers $\tt x_1$ to $\set{>}$, $\tt x_2$ to $\set{=}$. 

%% A thread-abstracted configuration satisfies a symbolic configuration if
%% \begin{itemize}
%% \item it satisfies program counter, observer state, and local variables %\bjcom{Say that program counter, and local variables ...}
%% \item
%%   For each cell $\cell$ in the heap of the configuration, there is
%%   a fragment $\tuple{\inputtag,\outputtag}$ in the set of fragments, such
%%   that $\cell \lhd \inputtag$ and ${\tt next}(\cell) \lhd \outputtag$.
%%   \bjcom{check the case when $\cell$ points to null etc.}
%% \end{itemize}

In the  verification, we must compute a symbolic representation
that is satisfied by all reachable program configurations (recall that
program configurations include the state of the observer).
This invariant is obtained by an abstract-interpretation-based
fixpoint procedure, which starts
from a representation of the set of initial configurations, and
thereafter repeatedly performs
postcondition computations that extend the
symbolic representation by the effect of any execution step of the program,
until convergence.
This procedure is presented in Section~\ref{subsect:postcond}.




