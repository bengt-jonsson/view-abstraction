\section{Overview}
\label{sec:overview}

  In this section, we illustrate our technique by using it to prove correctness, in
the sense of linearizability, of
a concurrent data structure implementation which uses skiplists.
We consider an implementation of a set data structure, which operates on
a shared heap which represents a skiplist. Here, we consider the
Lock-Free Concurrent Skiplist, described in~\cite[Section 14.4]{ArtOfMpP}.
To the best of our knowledge, no existing automated verification technique
has succeeded in verifying functional correctness of concurrent
skiplist algorithms.
We have automatically verified both lock-based and lock-free skiplist-based algorithms using fragment
abstraction, as reported in Section~\ref{section:experiments}.


\subsection{The Skiplist Algorithm}
%\todo[inline]{Comment from BJ to Quy: begin with context: Which algorithm are
%  we showing? what does that algorithm do?}
  
To illustrate our approach, we use the lock-free concurrent lock-free skiplist algorithm~\cite[Section 14.4]{ArtOfMpP}. Informally, a skiplist consists of a collection of sorted linked lists, each of which is located at a {\em level}, ranging from $1$ up to a maximum value. Each skiplist node has a key value and participates in the lists at levels $1$ up to its {\em height}.
The skiplist has sentinel head and tail nodes with maximum heights and key values $-\infty$ and $+\infty$, respectively.
%Figure~\ref{sl} shows a skiplist with maximum height of 3.
The lowest-level list (at level $1$) constitutes an ordered list of all nodes
in the skiplist. Higher-level lists are increasingly sparse sublists of the
lowest-level list, and serve as shortcuts into lower-level lists.
%\bjcom{This does not hold in our case, so I think it may confuse. Start rather with the reason for higher-level (they are shortcuts). You might want to find some
%  good text on this topic as inspiration}
%\begin{figure}
%\center  
% \input skiplist
%  \caption{A concrete shape example of 3-level skiplist}
%\label{sl}
%\end{figure}

%\bjcom{Say which are the methods}
There are three main methods of the algorithm namely $\tt add$, $\tt contains$ and $\tt remove$. All methods rely on a method $\tt find$ to search for a given key. In this section, we shortly describe the $\tt find$ and $\tt add$ methods.
Figure~\ref{sl-code:fig} shows code for these methods.
%\bjcom{Start with the most important property of the method. What does it do?}
\input slcode

%\todo[inline]{I am working with add and remove methods}

%\todo[inline]{To Quy: PLease update the below text asap}

%\bjcom{Say how the list looks, and the role of marked nodes}
Each heap cell has a {\tt key} field, a $\tt marked$ field which is true if the node has been logically removed, and an array of {\tt next} pointers, indexed from
$1$ upto its {\tt height}.
The $\tt find$ method traverses the list at decreasing levels, starting from
the maximum level. During the traversal, the two pointer variables $\tt pred$ and $\tt curr$ are advanced until $\tt pred$ points to a node with the largest key on that level smaller than $\tt x$, recording the
obtained values of  $\tt pred$ and $\tt curr$ into the arrays
$\tt preds$ and $\tt succs$ at the current level, and thereafter
continuing one level below.
During the traversal, the method removes marked nodes at the current level
(lines 15 to 19) using a $\tt compareAndSet$ command. The $\tt compareAndSet$ statement tests the expected reference and mark values, and if both tests succeed, replaces them with updated reference and mark values.  
The $\tt compareAndSet$ command returns a Boolean indicating whether the value changed.
%% Once an unmarked $\tt curr$ is found (line 20), it is tested to see if its key is less than the target key. If so, $\tt pred$ is advanced to $\tt curr$. Otherwise, $\tt curr$'s key is greater than or equal to $\tt x$, so the current value
%% of $\tt pred$ is the target node's immediate predecessor. The method breaks out of the current level search loop, saving the current values of $\tt pred$ and $\tt curr$. It proceedsd int by this way until reaching the bottom level.

The $\tt add$ method uses $\tt find$ to check whether a node with key $\tt x$ is already in the list. If an unmarked node with key $\tt x$ is found in the bottom-level list, the $\tt add$ method returns $\tt false$. If no node is found, then the next step is to try to add the new node into the list. It is performed by linking it into the bottom-level list between the $\tt preds[0]$ and $\tt succs[0]$ nodes returned by $\tt find$. using a $\tt compareAndSet$ to set the reference while validating that these nodes still refer one to the other and have not been removed from the list (line 16). If $\tt compareAndSet$ fails, the method call will restart. Otherwise, the node is added into the list. Thereafter, the $\tt add$ proceeds with linking the new node into increasingly higher levels.
 
%The $\tt remove$ method also use $\tt find$ to determine whether an unmarked node with a matching key $\tt k$ is in the bottom-level list. If no node is found in the bottom-level list, or the node with a matching key is marked, the method returns false. If an unmarked node is found, then the method logically removes the associated key from the list, and prepares it for physical removal. First, starting from the topLevel, all links up to and not including the bottom-level link are marked . If the link is found to be marked the method moves on to the next-level link. Otherwise, the marking attempt must be repeated. Once all levels but the bottom one have been marked, the method marks the bottom levels next reference. Before returning true, the $\tt find$ method is called again. This call physically removes all links to the node it is searching for if that node is already logically removed.

%For each level, it attempts to set the predecessor, if it refers to the valid successor, to the new node . If successful, it moves on to the next level. If unsuccessful, the $\tt find$ method is called again to find a new valid position. 

%We use $\tt compareAndSet$ to set the reference while validating that these nodes still refer one to the other and have not been removed from the list. If the $\tt compareAndSet$ fails, the call restarts. 
%Figure~\ref{ts-stack:fig} shows a simplified version of the Timestamped Stack (TS stack), where we have omitted the check for emptiness in the ${\tt pop}$ method, and the optimization using ${\tt push}$-${\tt pop}$ elimination. These features are included in the full version of the algorithm, described in Appendix XXX, that we have verified automatically.
%
%The algorithm uses an array of singly-linked lists (SLLs), one for each thread, accessed via the thread-indexed array ${\tt pools[maxThreads]}$ of pointers to the first cell of each list. The ${\tt init}$ method initializes each of these pointers to $\nullconst$. Each list cell contains a data value, a timestamp value, a ${\tt next}$ pointer, and a boolean flag ${\tt mark}$ which indicates whether the node is logically removed from the stack. Each thread pushes elements to ``its own'' list, and can pop elements from any list.
%
%A ${\tt push}$ method for inserting a data element ${\tt d}$ works as follows: first, a new cell with element ${\tt d}$ and minimal timestamp ${\tt -1}$ is inserted at the beginning of the list indexed by the calling thread (line 1-3). After that, a new timestamp is created and assigned (via the variable ${\tt t}$) to the ${\tt ts}$ field of the inserted cell (line 4-5).
%%\bjcom{Quesion to Quy: What happens when pop methods sees uninitialized timestamps?} \quycom{uninitialized time is -1, its smaller than any other time.}
%Finally, the method unlinks (i.e., physically removes) all cells that are reachable (through a sequence of $\tt next$ pointers) from the inserted cell and whose ${\tt mark}$ field is ${\tt true}$; these cells are already logically removed. This is done by redirecting the $\tt next$ pointer of the inserted cell to the first cell with a $\false$ $\tt mark$ field, which is
%reachable from the inserted cell.
%
%A ${\tt pop}$ method first traverses all lists, finding in each list
%the first cell whose ${\tt mark}$ field is ${\tt false}$(line 8), and letting the variable ${\tt youngest}$ point to the most recent such cell
%(i.e., with the largest timestamp) (line 1-11).
%A compare-and-swap (CAS) is used
%to set the ${\tt mark}$ field of this youngest cell to $\true$,
%thereby logically removing it.
%This procedure will restart if the CAS fails. After the youngest cell has been removed, the method will unlink all cells, whose ${\tt mark}$ field is ${\tt true}$,
%that appear before (line 17-19) or after (line 20-23) the removed cell.
%Finally, the method returns the ${\tt data}$ value of the removed cell.
%%\todo[inline]{Comments by Bengt: How can you return empty? Also, I think you
%% need to explain the extra removal of marked cells better} \quycom{this code is a simplified version without emptiness checking. If we add emptiness part, the code will be longer and more complicated}


\subsection{Specifying the Correctness Criterion of Linearizability}
In our verification, we establish that the skiplist algorithm of
Figure~\ref{sl-code:fig} is correct in the sense that it is a
linearizable implementation of a set data structure.
Linearizability intuitively states that
each operation on the data structure can be considered as being
performed atomically at some point (called the {\em linearization point (LP)})
between its invocation and return~\cite{HeWi:linearizability}.
% 
We specify linearizability by the technique of
{\em observers}~\cite{AHHR:integrated}, including its subsequent
extensions~\cite{BEEH:icalp15,HSV:concur13,Quy:sas16}.
In the case of our skiplist algorithm,
LPs can be associated to fixed statements in the code. The user then instruments
these statements to announce the corresponding set operation.
For instance, the LP of a successful {\tt add} operation is at line 16 of the $\tt add$ method, whereas the linearization point of an unsuccessful {\tt add} operation is at line 7 of the $\tt add$ method.  %\bjcom{Quy: Please insert a text of
%how some LP is inserted into the code of Figure~\ref{sl-code:fig}}.
\bjcom{Put these LPs as dots in the code}
Having instrumented methods at LPs, we
then consider an arbitrary concurrent program consisting of
an arbitrary collection of threads, each of which executes some method call.
We must check that the concurrent execution of such a program generates
(through its instrumentation) a sequence of operations which satisfies the
semantics of the set data structure. 
This check is performed by an {\em observer}, which 
monitors the sequence of operations that is announced by the
instrumentation, and reports when it violates the semantics of the set
data structure by moving to an accepting ``error state''.
Observers are described in more detail in Section~\ref{subsect:observers}.

%% typically by affixing
%%   linearization points to particular statements, or in more complex cases by
%%   light-weight instrumentation using the approach of
%%   controllers~\cite{Quy:sas16}.

%% For many data structure implementations, LPs can be statically
%% affixed to particular statements in method implementations,
%% implying that correctness can be formulated as constraints on the ordering of the
%% occurrences of LPs in any program execution.
%% a concept of data-independence, these constraints can be formulated as
%% a number of samll observers, which are automata that recognize violations
%% of these ordering requirements~\cite{AHHR:integrated}.
%% does not LPs cannot be affixed to particular statements. For instance,  two overlapping
%% push operations may have to be linearized in different orders depending
%% on how their corresponding later pop operations are ordered in time.
%% not be determined during their invocation, but only later
%% when the ordering between the two corresponding pop operations is determined.
%% Instead, we use a recent technique of~\cite{BEEH:icalp15,HSV:concur13}, who show
%% that for stacks and queues,
%% linearizability can be precisely specified by a small number of
%% constraints on the ordering of call and return actions of the methods,
%% without any reference to LPs.


%% \begin{wrapfigure}{r}{.55\textwidth}
%% {\em Observers} are
%% finite automata extended with a finite set of {\em registers}
%% that assume values in $\intgrs$, which are 
%% nondeterministically initialized with arbitrary values, which never change
%% during a run of the observer. 
%% The observer accepts a trace if, for {\em some} initial values of the
%% registers, the trace can  be processed in such a way that
%% an accepting state is reached.
%% In other words, the observer is defined in such a way that it accepts precisely those
%% traces that do {\em not} belong to the behavior
%% of the data structure.
%% %% Observers can be used to give {\it exact} specifications of
%% %% the behaviors of data structures such as sets, queues, and stacks.
%% %
%% %
%% Fig.~\ref{set:observer:fig}
%% depicts an observer that accepts the
%% sequences of operations that do {\em not} conform to the semantics of a set
%% data structure.

To verify that no execution of the program may cause the observer to
accept, we form
%% as in the automata-theoretic approach~\cite{VW:modelchecking}
%% (adapted in~\cite{AHHR:integrated}),
the cross-product of the program  and the corresponding
observer, in which the observer synchronizes with the program on the operations
that are announced at LPs. This reduces the
problem of checking linearizability to a reachability problem: checking
that, in the cross-product, the observer cannot reach an accepting state.

In conclusion, verifying linearizability for skiplist-based set implementations
requires the user to associate linearization points with statements in the
methods of the implementation: the remaining steps are then automated.
We remark that this user annotation is not necessary when verifying
linearizability of stack and queue implementation. 
This is achieved by exploiting recent results saying that
linearizability for stacks and queues can be precisely specified by
observers that process the sequence of call and return actions of
methods,  instead of the user-supplied LPs~\cite{BEEH:icalp15,HSV:concur13}.
We use this technique for stacks and queues in Section XXX.

%% In more complex cases, we use
%%   light-weight instrumentation using the approach of
%%   controllers~\cite{Quy:sas16}, which puts a small burden on
%%   the verifier. This is used in Section XXX for the verification of some
%%   implementations of sets based on singly-linked lists.

%% For concurrent data structures, one must be careful to 
%% expressed it exactly in terms of allowed sequences of calls
%% and returns of different methods. For completeness, we here
%% present how this can be done for a stack implementation, as shown
%% in~\cite{BEEH:icalp15}.
%% show how this can be done using observers, introduced in~\cite{AHHR:integrated}. 

%% Let $\calltwo{\tt push}{\tt d}$ and $\returntwo{\tt push}{\tt d}$ denote the
%% action of calling or returning from a ${\tt push}$ method with data parameter ${\tt d}$.
%% Let $\calltwo{\tt pop}{\tt d}$ and $\returntwo{\tt pop}{\tt d}$ denote the
%% action of calling or returning from a ${\tt pop}$ method which returns
%% the value ${\tt d}$.
%% %% Let a call action on an operation $\mname(\data)$ be denoted
%% %% $\calltwo{\mname}{\data}$ and a return action be denoted
%% %% $\returntwo{\mname}{\data}$.
%% Constraints on the ordering of such actions can be
%% checked by an {\em observer}.
%% %
%% Observers are
%% finite automata extended with a finite set of {\em registers} whose values range
%% over the domain of data values.
%% %
%% At initialization,
%% the registers are nondeterministically
%% assigned arbitrary distinct data values, which never change
%% during a run of the observer. 
%% %
%% %% Formally, an observer $\observer$ is a tuple
%% %% $\otuple$ where $\ostateset$ is a finite set 
%% %% of {\it observer states} including the 
%% %% {\it initial state} $\oinitstate$ and
%% %% the {\it accepting state} $\oaccstate$, 
%% %% a finite set $\ovarset$  
%% %% of {\it registers}, and $\otransitionset$ is a finite
%% %% set of {\it transitions}.
%% %% %
%% %% %
%% %% Transitions are of the form 
%% %% $\tuple{\ostate_1,\mname(\inxvar,\outxvar),\ostate_2}$ where 
%% %% $\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
%% Transitions are labeled by actions parameterized by registers.
%% %% is the value of a register parameterized on registers.%
%% The observer processes a sequence of actions, one action at a time.
%% If there is a transition, whose label, after replacing registers by their
%% values, matches the action, such a transition is performed; otherwise
%% the observer remains in its current state.
%% %% %
%% %% If there is no
%% %% such transition, 
%% %
%% The observer accepts a sequence of actions
%% if, for some initial assignment of data values to its registers, the sequence
%% can be processed in such a way that an accepting state is reached.
%% The observer for a particular constraint is defined in such a way that it
%% accepts precisely those sequences  that do {\em not} satisfy the constraint.


%% \begin{figure}
%% \center 

%% \begin{tikzpicture}[]

%% \node(s0)[draw,line width=0.5pt,circle,text=black]
%% {$s_0$};

%% \node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(40pt,50pt)$) {$s_2$};



%% \node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(80pt,0pt)$) {$s_1$};

%% \node(init)[line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(-30pt,0pt)$) {};
%% \draw[->,>=stealth,line width=0.5pt] (init) to 
%% node[left=5pt,pos=0.5] {} 
%% (s0);

%% \node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
%% at ($(s0.center)+(80pt,0pt)$) {};


%% \draw[->,>=stealth,line width=0.5pt,] (s0) to 
%% node[above=-1pt,pos=0.5] {\footnotesize${\tt ret \; pop}(x)$} 
%% (s1);

%% \draw[->,>=stealth,line width=0.5pt] (s0) to 
%% node[left=5pt,pos=0.5] {\footnotesize${\tt call \; push}(x)$} 
%% (s2);

%% \draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s0) to 
%% node[above=-15pt,pos=0.5] {\footnotesize${\tt call \; pop}(x)$} 
%% (s0);

%% %% \draw[->,>=stealth,line width=0.5pt, in=50,out=-30,loop] (s2) to 
%% %% node[right=2pt,pos=0.5] {\footnotesize${\tt act(x)}$} 
%% %% (s2);


%% %\node[]
%% %at ($(s2.center)+(60pt,20pt)$) {\footnotesize${\tt call \; push}(x)+$};
%% %\node[] at ($(s2.center)+(60pt,10pt)$) {\footnotesize${\tt ret \; push}(x)+$};
%% %\node[] at ($(s2.center)+(60pt,0pt)$) {\footnotesize${\tt call \; pop}(x)+$};
%% %\node[] at ($(s2.center)+(60pt,-10pt)$) {\footnotesize${\tt ret \; pop}(x)$};

%% \end{tikzpicture}
%% \caption{An observer recognizing sequences in which ${\tt pop}$ for some data value returns although no
%%   corresponding ${\tt push}$ has been previously called. The observer has one register $\tt x$.}
%% \label{fig:nonpopobserver:fig}
%% \end{figure}

%% %\todo[inline]{Quy: make a picture of this observer, and describe intuitively
%% %  how it works}


%% %\todo[inline]{The observer of Fig. 2 is not correct: You must also include
%% %  call push(x) which goes to a sink state. Moreover, you must explain that you
%% %  assume differentiated traces, Plese go over the observer again. Also mark
%% %  the initial location}

%% As an example, Figure~\ref{fig:nonpopobserver:fig} shows an observer for
%% Constraint 1, which checks
%% that a ${\tt pop}$ method must not return before any ${\tt push}$ method
%% with the same data value has been called.
%% %% The exact form of observers for these constraints represent them in
%% %% terms of ordering of call an return actions.
%% Observers for Constraints 2 and 3 are similar to that for Constraint 1.
%% The observer for Constraint 4 is more complex,
%% and shown in Figure~\ref{fig:lifostack:fig}.
%% It has two registers $\tt x_1$ and $\tt x_2$. Let $\tt d_1$ and $\tt d_2$
%% denote their initial values.
%% In a sequence of actions that leads to the accepting state $s_6$, a
%% ${\tt push(d_1)}$ must be linearized before a ${\tt push(d_2)}$, since
%% ${\tt push(d_1)}$ returns before ${\tt push(d_2)}$ is called; however
%% a ${\tt pop(d_1)}$ returns in a situation where the number of returned
%% ${\tt push(d_2)}$ methods exceeds the number of called
%% ${\tt pop(d_2)}$ methods, thereby violating LIFO ordering.
%% %% ${\tt pop(d_2)}$ which leaves some data value $\tt d_2$ on top of
%% %% $\tt d_1$ in the stack, also violating LIFO ordering.
%% %% In the observer, the action which leads to an accepting state $\tt ret \; pop(x_1)$ happens after sequences of actions namely $\tt call \; push(x_2)$, $\tt ret \; push(x_2)$, $\tt call \; pop(x_2)$ in which the pair $\tt ret \; push(x_2) / call \; pop(x_2)$ is always after $\tt ret \; push(x_2)$ . The $\tt call \; push(x_1)/ret \;push(x_1)$ happens before all the sequences. Intuitively, the observer specifies that a data cannot be popped from the stack if there is always at least an different data above it in the Stack (regardless of how linearize the execution). 
%% %\todo[inline]{The explanation of Figure 3 to be improved}
%% \begin{figure}
%% \center 

%% \begin{tikzpicture}[]

%% \node(s0)[draw,line width=0.5pt,circle,text=black]
%% {$s_0$};


%% \node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(60pt,0pt)$) {$s_1$};

%% \node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(120pt,0pt)$) {$s_2$};

%% \node(s3)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(180pt,0pt)$) {$s_3$};

%% \node(s4)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(240pt,0pt)$) {$s_4$};

%% \node(s6)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(300pt,0pt)$) {$s_6$};

%% \node(s5)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(240pt,-100pt)$) {$s_5$};

%% \node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
%% at ($(s0.center)+(300pt,0pt)$) {};


%% \draw[->,>=stealth,line width=0.5pt,] (s0) to 
%% node[above=-1pt,pos=0.5] {\tiny${\tt call \; push}(x_1)$} 
%% (s1);
%% \draw[->,>=stealth,line width=0.5pt,] (s1) to 
%% node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_1)$} 
%% (s2);

%% \draw[->,>=stealth,line width=0.5pt,] (s2) to 
%% node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
%% (s3);

%% \draw[->,>=stealth,line width=0.5pt,] (s3) to 
%% node[above=-1pt,pos=0.5] {\tiny${\tt call \; pop}(x_1)$} 
%% (s4);

%% \draw[->,>=stealth,line width=0.5pt,] (s4) to 
%% node[above=-1pt,pos=0.5] {\tiny${\tt ret \; pop}(x_1)$} 
%% (s6);

%% \draw[->,>=stealth,line width=0.5pt, in=50,out=130,loop] (s2) to 
%% node[above=2pt,pos=0.5] {\tiny${\tt call \; push}(x_2)$} 
%% (s2);

%% \draw[->,>=stealth,line width=0.5pt,out=-35,in=35] (s4) to 
%% node[right=-2pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
%% (s5);

%% \draw[->,>=stealth,line width=0.5pt,out=145,in=-145] (s5) to 
%% node[left=-2pt,pos=0.5] {\tiny${\tt call \; pop}(x_2)$} 
%% (s4);

%% \node(init)[line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(-30pt,0pt)$) {};
%% \draw[->,>=stealth,line width=0.5pt] (init) to 
%% node[left=5pt,pos=0.5] {} 
%% (s0);

%% \node(s7)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s5.center)+(-180pt,-2pt)$) {$s_7$};

%% \draw[->,>=stealth,line width=0.5pt] (s0) to 
%% node[above = -2pt,pos=0.5, sloped] {\tiny${\tt \neg{call \;push(x_1)}}$} 
%% (s7);
%% \draw[->,>=stealth,line width=0.5pt] (s1) to 
%% node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg {ret \; push(x_1)}}$} 
%% (s7);
%% \draw[->,>=stealth,line width=0.5pt] (s2) to 
%% node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; push(x_2) \wedge \neg{ret \; push(x_2)}}}$} 
%% (s7);
%% \draw[->,>=stealth,line width=0.5pt] (s3) to 
%% node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; pop(x_1)}}$}  
%% (s7);
%% \draw[->,>=stealth,line width=0.5pt] (s4) to 
%% node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{ret \; pop(x_1) \wedge \neg{ret \; pop(x_2)}}}$}  
%% (s7);
%% \draw[->,>=stealth,line width=0.5pt] (s5) to 
%% node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; ret(x_2)}}$}  
%% (s7);

%% \end{tikzpicture}
%% \caption{An observer recognizing LIFO violations. The registers are $\tt x_1$ and $\tt x_2$. For each action $\tt act$ parameterized by $\tt x_1$ or $\tt x_2$, the label $\tt \neg act(x_i)$ is the union of  all actions parameterized by $\tt x_1$ or $\tt x_2$ except for $\tt act(x_i)$.  
%% }
%% \label{fig:lifostack:fig}
%% \end{figure}


%% To this end, define a {\em program} to consist 
%% of an arbitrary number of concurrently executing threads,
%% %(Appendix~\ref{planguage:section} contains
%% %the language syntax used in our tool).
%% %
%% each of which executes an add or remove method with some data value as parameter.
%% We assume that the data structure has been initialized
%% by the ${\tt init}$ method prior to the start of program execution.
%% We must verify that any execution of such a program satisfies the 
%% constraints.

%% Instrumentation to check conditions 
%% (i) - (iii) is added automatically, reducing the verification to a
%% reachability problem, i.e., to verify that no configuration that violates
%% conditions (i) - (iv) can be reached.

%% The addition of observers reduces the problem of checking linearizability to
%% checking control state reachability for the composition of an arbitrary
%% program and an observer.
%% \begin{enumerate}[(i)]
%% \item each method invocation generates a sequence of linearization events,
%%   of which only the last one may change the state of the observer,
%% \item
%%   the linearization event which is generated last
%%   is consistent with the call parameters and return value of the method,
%%   and
%% \item  that the sequence of linearization
%%   events cannot cause the observer to reach an accepting state.
%% \end{enumerate}
%% Tasks (i) and (ii) can be verified by standard instrumentations of
%% methods, which we will not further elaborate on here.
%% is added automatically, reducing the verification to a
%%  and our verification by standard techniques, as well as
%% the standard way, and verify condition (iv) by checking that the observer cannot
%% reach an accepting state.

\subsection{Verification by Fragment Abstraction}

In the actual verification, we must compute a symbolic representation
of an invariant that is satisfied by all reachable configurations of
the cross-product of the program  and an observer.
The verification must address the challenges of an unbounded domain of
data values, an unbounded number of concurrently executing threads, and an
unbounded heap.
For this, we have developed a novel shape representation, called
{\em fragment abstraction}, which we combine with
data abstraction and thread abstraction.
Let us illustrate how fragment abstraction applies to the skiplist
algorithm.
\begin{figure}
\center  
 \input skiplistshape  
 \caption{A concrete shape of 3-level skiplist with three threads}
\label{sl-shape}
\end{figure}

%\todo[inline]{Quy: you must provide example heaps, etc. before I can continue
%  writing}

Figure~\ref{sl-shape} shows an example state of the heap of the
skiplist algorithm with three levels. Each heap cell is shown with the values of its fields. \bjcom{Add a legend, showing the layout of fields, as in previous papers}
In addition, each cell is labeled by the
pointer variables that point to it; we use ${\tt lvar(i)}$ to denote the local
variable ${\tt lvar}$ of thread $\thread_i$.
In the heap state of Figure~\ref{sl-shape}, thread $\thread_1$ is trying to add a new node with key $9$, and has reached line $8$ of the $\tt add$ method.  Thread $\thread_2$ is also doing the same thing as $\thread_1$ and it has done its first iteration of the loop in $\tt find$ method in line $25$.
%\bjcom{Where is $\thread_2$'s new node?}
%\bjcom{There are strange things: the Figure 2 has the variable $\tt curr$ as an
%  array (it is not), and does not show the arrays $\tt preds$ and $\tt succs$.
%  Could you please fix this?}

One of the main properties of the algorithm is that the $\tt pred$ variable arrives at the bottom-level list at a node before, and never after, the target node. If the node is physically removed before the $\tt find$ starts, then it will not be found.

%The heap consists of a set of singly linked lists (SLLs), each of which
%is accessed from a pointer in the array ${\tt pools[maxThreads]}$
%in a configuration when % Quy write from here
%it is accessed concurrently by three threads $\thread_1$, $\thread_2$, and $\thread_3$. The heap consists of three SLLs accessed from the three pointers $\tt pools[1]$, $\tt pools[2]$, and $\tt pools[3]$ respectively. Each heap cell is
%shown with the values of its fields, using the layout shown to the right in
%Figure~\ref{fig:tsshape}.
%In addition, each cell is labeled by the
%pointer variables that point to it. We use ${\tt lvar[i]}$ to denote the local
%variable ${\tt lvar}$ of thread $\thread_i$.
%
%In the heap state of Figure~\ref{fig:tsshape},
%thread $\thread_1$ is trying to push a new node with data value $4$, pointed by its local variable $\tt new$, having reached line 3.
%Thread $\thread_3$ has just called the ${\tt push}$ method.
%Thread $\thread_2$ has reached line 12 in the execution of the ${\tt pop}$ method,  and has just assigned ${\tt youngest}$ to the first node in the list
%pointed to by $\tt pools[3]$ which is not logically removed (in this case it is the last node of that list).
%Not shown in Figure~\ref{fig:tsshape} is the configuration of the
%observer.
%In this case, the
%observer is the one in Figure~\ref{fig:lifostack:fig}, which has two registers
%$\tt x_1$ and $\tt x_2$, which are assigned the values $4$ and $2$,
%respectively.

%\begin{figure}
%\center  
% \input skiplistabs  
% \caption{Fragment abstraction of the skiplist algorithm}
%\label{sl-abs}
%\end{figure}

Our verification technique is based on a symbolic representation of program
configurations, which combines a thread abstraction, a
data  abstraction, and a shape abstraction.
\begin{itemize}
  \item
Our {\em thread abstraction} adapts the thread-modular approach by representing only the view of single, but arbitrary, thread $\thread$. Such a view consists of
  \begin{inparaenum}[(i)]
  \item the local state of thread $\thread$,
  \item the part of the heap that is accessible to thread $\thread$ via pointer variables (local to $\thread$ or global), and
  \item the state of the observer.
  \end{inparaenum}
\item
We use a novel {\em shape abstraction}, which represents the part of the heap
that is accessible to $\thread$ by a set of {\em fragments}. 
%% The part of the heap that is accessible to a thread $\thread$ is represented by  
%% a set of {\em fragments}.
A fragment consists of a pair of heap cells that are connected by a pointer.
For each of these cells, the fragment represents
%% Each fragment is an abstraction of a pair of
%% nodes in the heap that are connected by a pointer. The abstraction represents
\begin{inparaenum}[(i)]
\item the values of its non-pointer fields,
%%  in the pair of nodes (under the empoyed  data abstraction),
\item the pointer variables (either local to $\thread$ or global)
  that point to the cell, and
\item global reachability information, which expresses how
  each node in the pair can reach to and be reached from
  (by following a chain of pointers) a finite set of globally significant
  heap cells. A cell is globally significant if it is pointed to by a
  global pointer variable or if its 
  {\tt data} field has the  same value as the register of the set observer.
\end{inparaenum}
In our fragment abstraction, a set of fragments represents the set of heap
structures in which each pair of pointer-connected nodes is represented by some
fragment in the set.
Intuitively, a set of fragments describes the set of heaps that can be formed by
``piecing together'' fragments in the set. This ``piecing together'' must
be both locally consistent (appending only fragments that agree on their
common node), and globally consistent (respecting the global reachability
information).
\item
A natural data abstraction is applied to the non-pointer local
variables of the thread, the non-pointer fields of each fragment, as well as
the observer registers. For our
skiplist algorithm, fields that range over small finite domains are represented
by their concrete values, whereas the observer registers and fields that range over the domain of data values
%% , as well as the observer registers,
are represented by constraints over their relative ordering
(wrp.\ to the order $<$).
\end{itemize}
Our combination of fragment abstraction, thread abstraction, and data
abstraction results in a finite abstract domain, thereby guaranteeing
termination of our analysis.

%% Let a {\em local symbolic configuration} be an abstraction of the program
%% counter and local data variables of an arbitrary thread $\thread$.
%% Our symbolic representation of a set of program configurations consists of
%% a mapping from a set of local symbolic configurations,
%% which maps each local symbolic configuration in its domain to a set of fragments.
%% A global configuration satisfies a symbolic representation $\symbrep$
%% if the view of each thread $\thread$ satisfies some local symbolic
%% configuration in the domain of $\symbrep$, which is mapped to a set
%% of fragments, which represents the heap that is accessible to $\thread$.

\begin{figure}
\center
	\input skiplistabs
\caption{Fragment abstraction of skiplist algorithm}
\label{fig:skiplistabs}
\end{figure} 

Figure~\ref{fig:skiplistabs} shows a set of fragments that represents the
part of the heap that is accessible to
$\thread_1$ in the configuration in Figure~\ref{sl-shape}. We assume that
the value of the observer register is $9$. 
There are $10$ fragments, named $\frag_1$, \ldots , $\frag_{10}$. Two of
these ($\frag_5$ and $\frag_9$) consist of a tag that points to $\dangconst$,
and the other consist of a pair of pointer-connected tags. The fragments $\frag_1$, \ldots , $\frag_{6}$ are abstracted from the level $1$, whereas the other fragments are abstracted from higher levels. As can be seen, the property where the $\tt pred$, in bottom level, never refers to an unmarked node whose key is greater than or equal to $\tt x$ is preserved by the abstraction. The data field of the tag which contains $\tt pred$ in fragments $\frag_9$ is smaller than $\tt x$.  In addition, there is no tag whose marked field is $\tt true$ and contains $\tt x$ in its reach to information, it means that the abstraction preservers the property where the node which is physically removed can not be found by $\tt find$ method.   
\bjcom{The below if for TS Stack. Should it be replaced or removed?}
\begin{itemize}
\item The $\tt data$ field of the tag (to the left) abstracts the data value
  $2$ to the set of observer registers with that value: in this case
  $\reg_2$.
\item The $\tt ts$ field (at the top) abstracts the timer value $15$ to
  the possible relations with $\tt ts$-fields of heap cells with the same
  data value as each observer registers. Recall that observer registers
  $\reg_1$ and $\reg_2$ have values $4$ and $2$, respectively. There are
  three heap cells with $\tt data$ field value $4$, all with a $\tt ts$
  value less than $15$. There is one heap cell with
  $\tt data$ field value $2$, having $\tt ts$ value $15$.
  Consequently, the abstraction of the $\tt ts$ field maps $\reg_1$ to
  $\set{>}$ and $\reg_2$ to $\set{=}$: this is shown as the mapping
  $\lambda_4$ in Figure~\ref{fig:tsviewshape}.
\item The $\tt mark$ field assumes values from a small finite domain and
  is represented precisely as in concrete heap cells
\end{itemize}
Above the top, the tag contains the thread-local and global pointer variables
that point to the cell, in this case $\tt youngest$ and $\tt n$.
At the bottom of the tag, the first row contains the global variables 
pointing to cells from which the cell can be reached, in this case
$\tt pools[3]$, as well as observer registers whose value is equal to the
${\tt data}$ field of a cell from which  the cell can be reached, in this
case $\reg_2$ (since the cell itself has the same data value as $\reg_2$).
The second row contains dual information: now for cells that can be reached
from the cell itself (this is again $\reg_2$).

\todo[inline]{Fix the issue with ${\tt pools[3]}$}

Each cell in the heap state of Figure~\ref{fig:tsshape} now satisfies
some tag in Figure~\ref{fig:tsshape}. Moreoever, each pair of pointer-connected
cells (where the pointed-to ``cell'' can also be $\dangconst$)
satisfies some fragment in Figure~\ref{fig:tsshape} in the obvious way.
Conversely, the set of fragments in
Figure~\ref{fig:tsshape} represents the set of heaps in which each pair of
pointer-connected cells satisfies one of its fragments. For instance, the
list pointed to by $\tt pools[3]$ is represented by the sequence of
fragments $\frag_4 \frag_5 \frag_6 \frag_7$.

\todo[inline]{Maybe we should omit the rest of this section?}
In order to obtain a complete representation of reachable program configurations,
we must also represent the local states of a thread. This is done in a standard
manner, by applying the same data abstraction as for heap cells. For instance,
the local state of thread $\thread_2$ corresponding to
Figure~\ref{fig:tsshape} is represented by a {\em local symbolic configuration}
that contains the values of the program counter and variable $\tt success$,
abstracts the value of ${\tt k}$ into the set $\set{{\tt me},{\tt ot}}$ and
applies the timestamp abstraction to ${\tt maxTS}$.

%% We are now ready to present our symbolic representation of a set of
%% reachable program configurations. 
%% %% It maps each local symbolic configuration to a set of fragments, which
%% %% is a symbolic representation of the set of corresponding reachable heaps.
%% Our symbolic representation is a mapping from a set of local symbolic
%% configurations, which maps each local symbolic configuration in its domain
%% to a set of fragments.
%% A global configuration satisfies a symbolic representation $\symbrep$
%% if the local state of each thread $\thread$ satisfies some local symbolic
%% configuration in the domain of $\symbrep$, which is mapped to a set
%% of fragments, which is satisfied by heap wrp.\ to thread $\thread$.


%% Our symbolic representation represents the reachable local states of a thread by
%% a finite set of {\em local symbolic configurations}, using our
%% data abstraction.
%% It maps each local symbolic configuration to a set of fragments, which
%% is a symbolic representation of the set of corresponding reachable heaps.
%% Thus, our symbolic representation is a mapping from a set of local symbolic
%% configurations, which maps each local symbolic configuration in its domain
%% to a set of fragments.
%% A global configuration satisfies a symbolic representation $\symbrep$
%% if the local stat of each thread $\thread$ satisfies some local symbolic
%% configuration in the domain of $\symbrep$, which is mapped to a set
%% of fragments, which represents the heap that is accessible to $\thread$ in
%% the global configuration. In the following, we explain more precisely the
%% local symbolic configurations, and our fragment abstraction.

In the  verification, we must compute a symbolic representation
that is satisfied by all reachable program configurations (recall that
program configurations include the state of the observer).
This invariant is obtained by an abstract-interpretation-based
fixpoint procedure, which starts
from a representation of the set of initial configurations, and
thereafter repeatedly performs
postcondition computations that extend the
symbolic representation by the effect of any execution step of the program,
until convergence.
This procedure is presented in Section~\ref{subsect:postcond}.





