\section{Overview}
\label{sec:overview-ts}

In this section, we illustrate our technique by using it to prove correctness, in
the sense of linearizability, of
a concurrent data structure implementation, namely the Timestamped Stack
of~\cite{ts-stack}. This algorithm operates on a shared heap which represents
an array of singly linked lists.
The verification of correctness for this algorithm and the related
Timestamped queue, also of~\cite{ts-stack}, has been challenging. Only
recently has the TS stack been verified by non-automated
techniques~\cite{bouajjani:icalp17:rep} using a non-trivial extension of
forward simulation, and the TS queue by been verified manually by a new technique
based on partial orders~\cite{ts-stack}.
We have verified both these algorithms automatically using fragment abstraction,
as reported in Section~\ref{section:experiments}.

\begin{figure}
\center  
 \input skiplistshape  
 \caption{A concrete shape of 3-level skipl-list with two threads}
\label{sl-shape}
\end{figure}

\input slcode

\subsection{The Skiplist Algorithm}
Figure~\ref{sl-code:fig} shows the skiplist find method. The method returns −1 if the item  $\tt x$ is not found. It traverses the SkipList using two pointer variables $\tt pred$ and $\tt curr$ which start from the head and at the highest level. It then proceeds in each level down the list, filling in $\tt preds$
and $\tt succs$ nodes that are repeatedly advanced until $\tt pred$ points to a node with
the largest value on that level that is strictly less than the target key. The method repeatedly snips out marked nodes from the
given level as they are encountered (Lines 15 to 19) using a $\tt compareAndSet$ statement. Once an unmarked $\tt curr$ is found (Line 20), it is
tested to see if its key is less than the target key. If so, $\tt pred$ is advanced to $\tt curr$. Otherwise, $\tt curr$’s key is greater than or equal to the $\tt x$, so the current value
of $\tt pred$ is the target node’s immediate predecessor. The find method breaks
out of the current level search loop, saving the current values of $\tt pred$ and $\tt curr$. It proceeds this way until it reaches the bottom level.
\todo[inline]{I am working with add and remove methods}
\subsection{The Timestamped Stack Algorithm}

\input tscode



%\todo[inline]{Provide a description of the algorithm}
%\todo[inline]{Quy: The line numbers for Pop are scrambled. Can you fix?
%Also, the variable {\tt i} at line 6 is initialized strangely. Can you fix?}
Figure~\ref{ts-stack:fig} shows a simplified version of the Timestamped Stack (TS stack), where we have omitted the check for emptiness in the ${\tt pop}$ method, and the optimization using ${\tt push}$-${\tt pop}$ elimination. These features are included in the full version of the algorithm, described in Appendix XXX, that we have verified automatically.

The algorithm uses an array of singly-linked lists (SLLs), one for each thread, accessed via the thread-indexed array ${\tt pools[maxThreads]}$ of pointers to the first cell of each list. The ${\tt init}$ method initializes each of these pointers to $\nullconst$. Each list cell contains a data value, a timestamp value, a ${\tt next}$ pointer, and a boolean flag ${\tt mark}$ which indicates whether the node is logically removed from the stack. Each thread pushes elements to ``its own'' list, and can pop elements from any list.

A ${\tt push}$ method for inserting a data element ${\tt d}$ works as follows: first, a new cell with element ${\tt d}$ and minimal timestamp ${\tt -1}$ is inserted at the beginning of the list indexed by the calling thread (line 1-3). After that, a new timestamp is created and assigned (via the variable ${\tt t}$) to the ${\tt ts}$ field of the inserted cell (line 4-5).
%\bjcom{Quesion to Quy: What happens when pop methods sees uninitialized timestamps?} \quycom{uninitialized time is -1, its smaller than any other time.}
Finally, the method unlinks (i.e., physically removes) all cells that are reachable (through a sequence of $\tt next$ pointers) from the inserted cell and whose ${\tt mark}$ field is ${\tt true}$; these cells are already logically removed. This is done by redirecting the $\tt next$ pointer of the inserted cell to the first cell with a $\false$ $\tt mark$ field, which is
reachable from the inserted cell.

A ${\tt pop}$ method first traverses all lists, finding in each list
the first cell whose ${\tt mark}$ field is ${\tt false}$(line 8), and letting the variable ${\tt youngest}$ point to the most recent such cell
(i.e., with the largest timestamp) (line 1-11).
A compare-and-swap (CAS) is used
to set the ${\tt mark}$ field of this youngest cell to $\true$,
thereby logically removing it.
This procedure will restart if the CAS fails. After the youngest cell has been removed, the method will unlink all cells, whose ${\tt mark}$ field is ${\tt true}$,
that appear before (line 17-19) or after (line 20-23) the removed cell.
Finally, the method returns the ${\tt data}$ value of the removed cell.
%\todo[inline]{Comments by Bengt: How can you return empty? Also, I think you
% need to explain the extra removal of marked cells better} \quycom{this code is a simplified version without emptiness checking. If we add emptiness part, the code will be longer and more complicated}

\todo[inline]{It would here be nice with some intuitive argument why the
  algorithm works}

\subsection{Specifying the Correctness Criterion of Linearizability}
In our verification, we establish that the TS stack algorithm of
Figure~\ref{ts-stack:fig} is is correct in the sense that it is a
linearizable implementation of a stack data structure.
Linearizability intuitively states that
each operation on the data structure can be considered as being
performed atomically at some point, called the {\em linearization point (LP)},
between its invocation and return~\cite{HeWi:linearizability}.
We specify linearizability using the technique of
{\em observers}~\cite{AHHR:integrated}. Observers act like monitors that
synchronize with the monitored concurrent programs over designated actions,
and report violations of the linearizability criterion. In our verification,
we use observers in two ways:
\begin{enumerate}
\item
  For concurrent implementations of stacks and queues,
  linearizability can be precisely specified by
  observers that synchronize on call and return actions of
  methods, as shown by~\cite{BEEH:icalp15,HSV:concur13}; this is done without
 any user annotation.
\item
  For sets, the verification requires the user to annotate how linearization
  points are placed in each method, typically by affixing
  linearization points to particular statements, or in more complex cases by
  light-weight instrumentation using the approach of
  controllers~\cite{Quy:sas16}; this puts a small burden on
  the verifier.
In our work, we use this technique to specify linearizability of
set implementations (see Section XXX).
%% typically by affixing
%%   linearization points to particular statements, or in more complex cases by
%%   light-weight instrumentation using the approach of
%%   controllers~\cite{Quy:sas16}.
\end{enumerate}
In our verification of TS stack, we use the first approach. 

%% For many data structure implementations, LPs can be statically
%% affixed to particular statements in method implementations,
%% implying that correctness can be formulated as constraints on the ordering of the
%% occurrences of LPs in any program execution.
%% a concept of data-independence, these constraints can be formulated as
%% a number of samll observers, which are automata that recognize violations
%% of these ordering requirements~\cite{AHHR:integrated}.
%% does not LPs cannot be affixed to particular statements. For instance,  two overlapping
%% push operations may have to be linearized in different orders depending
%% on how their corresponding later pop operations are ordered in time.
%% not be determined during their invocation, but only later
%% when the ordering between the two corresponding pop operations is determined.
%% Instead, we use a recent technique of~\cite{BEEH:icalp15,HSV:concur13}, who show
%% that for stacks and queues,
%% linearizability can be precisely specified by a small number of
%% constraints on the ordering of call and return actions of the methods,
%% without any reference to LPs.
In the case of a stack, we use observers to check constraints that intuitively express that
%\todo[inline]{Quy: Make sure that you have the right constraints. I just guessed that the below four are the right ones}
\begin{enumerate}
\item a data item must not be popped before it is pushed,
\item pop must not return ``empty'' if some data element was pushed but not
  popped,
\item a pushed data item must not be popped twice, and
\item any two data items must be popped in last in first out stack order.% \bjcom{Here we specify stack order}
\end{enumerate}
For a sequential data structure, it is straight-forward to construct observers
that check each of these constraints.
For concurrent data structures, one must be careful to 
expressed them exactly in terms of allowed sequences of calls
and returns of different methods. For completeness, we here
present how this can be done for a stack implementation, as shown
in~\cite{BEEH:icalp15}.
%% show how this can be done using observers, introduced in~\cite{AHHR:integrated}. 

Let $\calltwo{\tt push}{\tt d}$ and $\returntwo{\tt push}{\tt d}$ denote the
action of calling or returning from a ${\tt push}$ method with data parameter ${\tt d}$.
Let $\calltwo{\tt pop}{\tt d}$ and $\returntwo{\tt pop}{\tt d}$ denote the
action of calling or returning from a ${\tt pop}$ method which returns
the value ${\tt d}$.
%% Let a call action on an operation $\mname(\data)$ be denoted
%% $\calltwo{\mname}{\data}$ and a return action be denoted
%% $\returntwo{\mname}{\data}$.
Constraints on the ordering of such actions can be
checked by an {\em observer}.
%
Observers are
finite automata extended with a finite set of {\em registers} whose values range
over the domain of data values.
%
At initialization,
the registers are nondeterministically
assigned arbitrary distinct data values, which never change
during a run of the observer. 
%
%% Formally, an observer $\observer$ is a tuple
%% $\otuple$ where $\ostateset$ is a finite set 
%% of {\it observer states} including the 
%% {\it initial state} $\oinitstate$ and
%% the {\it accepting state} $\oaccstate$, 
%% a finite set $\ovarset$  
%% of {\it registers}, and $\otransitionset$ is a finite
%% set of {\it transitions}.
%% %
%% %
%% Transitions are of the form 
%% $\tuple{\ostate_1,\mname(\inxvar,\outxvar),\ostate_2}$ where 
%% $\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
Transitions are labeled by actions parameterized by registers.
%% is the value of a register parameterized on registers.%
The observer processes a sequence of actions, one action at a time.
If there is a transition, whose label, after replacing registers by their
values, matches the action, such a transition is performed; otherwise
the observer remains in its current state.
%% %
%% If there is no
%% such transition, 
%
The observer accepts a sequence of actions
if, for some initial assignment of data values to its registers, the sequence
can be processed in such a way that an accepting state is reached.
The observer for a particular constraint is defined in such a way that it
accepts precisely those sequences  that do {\em not} satisfy the constraint.


\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(40pt,50pt)$) {$s_2$};



\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(80pt,0pt)$) {$s_1$};

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(80pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\footnotesize${\tt ret \; pop}(x)$} 
(s1);

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[left=5pt,pos=0.5] {\footnotesize${\tt call \; push}(x)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt, in=-50,out=-130,loop] (s0) to 
node[above=-15pt,pos=0.5] {\footnotesize${\tt call \; pop}(x)$} 
(s0);

%% \draw[->,>=stealth,line width=0.5pt, in=50,out=-30,loop] (s2) to 
%% node[right=2pt,pos=0.5] {\footnotesize${\tt act(x)}$} 
%% (s2);


%\node[]
%at ($(s2.center)+(60pt,20pt)$) {\footnotesize${\tt call \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,10pt)$) {\footnotesize${\tt ret \; push}(x)+$};
%\node[] at ($(s2.center)+(60pt,0pt)$) {\footnotesize${\tt call \; pop}(x)+$};
%\node[] at ($(s2.center)+(60pt,-10pt)$) {\footnotesize${\tt ret \; pop}(x)$};

\end{tikzpicture}
\caption{An observer recognizing sequences in which ${\tt pop}$ for some data value returns although no
  corresponding ${\tt push}$ has been previously called. The observer has one register $\tt x$.}
\label{fig:nonpopobserver:fig}
\end{figure}

%\todo[inline]{Quy: make a picture of this observer, and describe intuitively
%  how it works}


%\todo[inline]{The observer of Fig. 2 is not correct: You must also include
%  call push(x) which goes to a sink state. Moreover, you must explain that you
%  assume differentiated traces, Plese go over the observer again. Also mark
%  the initial location}

As an example, Figure~\ref{fig:nonpopobserver:fig} shows an observer for
Constraint 1, which checks
that a ${\tt pop}$ method must not return before any ${\tt push}$ method
with the same data value has been called.
%% The exact form of observers for these constraints represent them in
%% terms of ordering of call an return actions.
Observers for Constraints 2 and 3 are similar to that for Constraint 1.
The observer for Constraint 4 is more complex,
and shown in Figure~\ref{fig:lifostack:fig}.
It has two registers $\tt x_1$ and $\tt x_2$. Let $\tt d_1$ and $\tt d_2$
denote their initial values.
In a sequence of actions that leads to the accepting state $s_6$, a
${\tt push(d_1)}$ must be linearized before a ${\tt push(d_2)}$, since
${\tt push(d_1)}$ returns before ${\tt push(d_2)}$ is called; however
a ${\tt pop(d_1)}$ returns in a situation where the number of returned
${\tt push(d_2)}$ methods exceeds the number of called
${\tt pop(d_2)}$ methods, thereby violating LIFO ordering.
%% ${\tt pop(d_2)}$ which leaves some data value $\tt d_2$ on top of
%% $\tt d_1$ in the stack, also violating LIFO ordering.
%% In the observer, the action which leads to an accepting state $\tt ret \; pop(x_1)$ happens after sequences of actions namely $\tt call \; push(x_2)$, $\tt ret \; push(x_2)$, $\tt call \; pop(x_2)$ in which the pair $\tt ret \; push(x_2) / call \; pop(x_2)$ is always after $\tt ret \; push(x_2)$ . The $\tt call \; push(x_1)/ret \;push(x_1)$ happens before all the sequences. Intuitively, the observer specifies that a data cannot be popped from the stack if there is always at least an different data above it in the Stack (regardless of how linearize the execution). 
%\todo[inline]{The explanation of Figure 3 to be improved}
\begin{figure}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=0.5pt,circle,text=black]
{$s_0$};


\node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(60pt,0pt)$) {$s_1$};

\node(s2)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(120pt,0pt)$) {$s_2$};

\node(s3)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(180pt,0pt)$) {$s_3$};

\node(s4)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,0pt)$) {$s_4$};

\node(s6)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(300pt,0pt)$) {$s_6$};

\node(s5)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(240pt,-100pt)$) {$s_5$};

\node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(300pt,0pt)$) {};


\draw[->,>=stealth,line width=0.5pt,] (s0) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; push}(x_1)$} 
(s1);
\draw[->,>=stealth,line width=0.5pt,] (s1) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_1)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,] (s2) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s3);

\draw[->,>=stealth,line width=0.5pt,] (s3) to 
node[above=-1pt,pos=0.5] {\tiny${\tt call \; pop}(x_1)$} 
(s4);

\draw[->,>=stealth,line width=0.5pt,] (s4) to 
node[above=-1pt,pos=0.5] {\tiny${\tt ret \; pop}(x_1)$} 
(s6);

\draw[->,>=stealth,line width=0.5pt, in=50,out=130,loop] (s2) to 
node[above=2pt,pos=0.5] {\tiny${\tt call \; push}(x_2)$} 
(s2);

\draw[->,>=stealth,line width=0.5pt,out=-35,in=35] (s4) to 
node[right=-2pt,pos=0.5] {\tiny${\tt ret \; push}(x_2)$} 
(s5);

\draw[->,>=stealth,line width=0.5pt,out=145,in=-145] (s5) to 
node[left=-2pt,pos=0.5] {\tiny${\tt call \; pop}(x_2)$} 
(s4);

\node(init)[line width=0.5pt,circle,text=black,anchor=center]
at ($(s0.center)+(-30pt,0pt)$) {};
\draw[->,>=stealth,line width=0.5pt] (init) to 
node[left=5pt,pos=0.5] {} 
(s0);

\node(s7)[draw,line width=0.5pt,circle,text=black,anchor=center]
at ($(s5.center)+(-180pt,-2pt)$) {$s_7$};

\draw[->,>=stealth,line width=0.5pt] (s0) to 
node[above = -2pt,pos=0.5, sloped] {\tiny${\tt \neg{call \;push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s1) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg {ret \; push(x_1)}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s2) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; push(x_2) \wedge \neg{ret \; push(x_2)}}}$} 
(s7);
\draw[->,>=stealth,line width=0.5pt] (s3) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; pop(x_1)}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s4) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{ret \; pop(x_1) \wedge \neg{ret \; pop(x_2)}}}$}  
(s7);
\draw[->,>=stealth,line width=0.5pt] (s5) to 
node[above=-2pt,pos=0.5,  sloped] {\tiny${\tt \neg{call \; ret(x_2)}}$}  
(s7);

\end{tikzpicture}
\caption{An observer recognizing LIFO violations. The registers are $\tt x_1$ and $\tt x_2$. For each action $\tt act$ parameterized by $\tt x_1$ or $\tt x_2$, the label $\tt \neg act(x_i)$ is the union of  all actions parameterized by $\tt x_1$ or $\tt x_2$ except for $\tt act(x_i)$.  
}
\label{fig:lifostack:fig}
\end{figure}

To verify that TS Stack is a correct linearizable implementation of a stack, we
must show that any concurrent execution of method calls satisfies the
above four constraints (1) -- (4).
To this end, define a {\em program} to consist 
of an arbitrary number of concurrently executing threads,
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
each of which executes a push or pop method with some data value as parameter.
We assume that the data structure has been initialized
by the ${\tt init}$ method prior to the start of program execution.
We must verify that any execution of such a program satisfies the four
constraints above. To verify a constraint, 
we form, as in the automata-theoretic approach~\cite{VW:modelchecking},
also adopted in~\cite{AHHR:integrated},
the cross-product of the program  and the corresponding
observer, where the observer synchronizes with the program on call and
return actions that are recognized by the observer. This reduces the
problem of checking each of the above constraints to the problem of checking
that the corresponding observer cannot reach an accepting state.

\todo[inline]{We may have to say that we check the trivial conditions that
  were checked by the monitor in SAS 16}
%% Instrumentation to check conditions 
%% (i) - (iii) is added automatically, reducing the verification to a
%% reachability problem, i.e., to verify that no configuration that violates
%% conditions (i) - (iv) can be reached.

%% The addition of observers reduces the problem of checking linearizability to
%% checking control state reachability for the composition of an arbitrary
%% program and an observer.
%% \begin{enumerate}[(i)]
%% \item each method invocation generates a sequence of linearization events,
%%   of which only the last one may change the state of the observer,
%% \item
%%   the linearization event which is generated last
%%   is consistent with the call parameters and return value of the method,
%%   and
%% \item  that the sequence of linearization
%%   events cannot cause the observer to reach an accepting state.
%% \end{enumerate}
%% Tasks (i) and (ii) can be verified by standard instrumentations of
%% methods, which we will not further elaborate on here.
%% is added automatically, reducing the verification to a
%%  and our verification by standard techniques, as well as
%% the standard way, and verify condition (iv) by checking that the observer cannot
%% reach an accepting state.

\subsection{Verification by Fragment Abstraction}

In the actual verification, we must compute a symbolic representation
of an invariant that is satisfied by all reachable configurations of
the cross-product of the program  and an observer.
The verification must address the challenges of an unbounded domain of
data values, an unbounded number of concurrently executing threads, and an
unbounded heap.
For this, we have developed a novel shape representation, called
{\em fragment abstraction}, which can also be combined with
data abstraction and thread abstraction.
Let us illustrate how fragment abstraction to the TS stack algorithm.

Figure~\ref{fig:tsshape} shows an  example heap state of TS stack.
The heap consists of a set of singly linked lists (SLLs), each of which
is accessed from a pointer in the array ${\tt pools[maxThreads]}$
in a configuration when % Quy write from here
it is accessed concurrently by three threads $\thread_1$, $\thread_2$, and $\thread_3$. The heap consists of three SLLs accessed from the three pointers $\tt pools[1]$, $\tt pools[2]$, and $\tt pools[3]$ respectively. Each heap cell is
shown with the values of its fields, using the layout shown to the right in
Figure~\ref{fig:tsshape}.
In addition, each cell is labeled by the
pointer variables that point to it. We use ${\tt lvar[i]}$ to denote the local
variable ${\tt lvar}$ of thread $\thread_i$.
%\todo[inline]{Quy: Use a better scheme for indexing local variables. Previously,
%  we used ${\tt var[2]}$, for variable ${\tt var}$ of thread 2. Maybe use it also here?}
%\todo[inline]{You must start by explaining the notation better. E.g., what means
%  a pointer variable that labels a node?}

In the heap state of Figure~\ref{fig:tsshape},
thread $\thread_1$ is trying to push a new node with data value $4$, pointed by its local variable $\tt new$, having reached line 3.
Thread $\thread_3$ has just called the ${\tt push}$ method.
Thread $\thread_2$ has reached line 12 in the execution of the ${\tt pop}$ method,  and has just assigned ${\tt youngest}$ to the first node in the list
pointed to by $\tt pools[3]$ which is not logically removed (in this case it is the last node of that list).
%% The local state of thread $\thread_2$
%% consists of the values of its program counter, the boolean variable ${\tt success}$, the pointer variables ${\tt youngest}$, ${\tt myTop}$, and ${\tt n}$,
%% and the timestamp variable ${\tt maxTS}$.
Not shown in Figure~\ref{fig:tsshape} is the configuration of the
observer.
%% The cross-product of program and observer also includes the configuration of the
%% observer. also In verification, we must also consider the observer.
In this case, the
observer is the one in Figure~\ref{fig:lifostack:fig}, which has two registers
$\tt x_1$ and $\tt x_2$, which are assigned the values $4$ and $2$,
respectively.

\begin{figure}
	\input tsshape
\caption{A possible heap state of TS stack with three threads.}
\label{fig:tsshape}
\end{figure} 

Our fragment abstraction represents a set of heap states by
a set of {\em fragments}. Each fragment consists of
a pair of node types (called {\em tags}) that are connected by a pointer.
A tag is an abstraction of a heap cell, which contains both 
\begin{enumerate}[(i)]
\item
  local information about its non-pointer fields, obtained
  using data abstraction if necessary, and about variables pointing to the
  cell, and
 \item
   global information, saying how
it can reach to and be reached from (by following a chain of pointers)
other heap cells that are pointed to by global variables, or cells whose
data field has the same value as some observer register.
This global information describes how the cell is positioned relative to
``important'' cells in the heap. Note that 
cells which carry the the same data value as some observer register are also
considered ``important'', since it is necessary to track their position in 
order to verify ordering constraints that are checked by the observer.
\bjcom{Do we need to explain this better?}
\end{enumerate}
Our fragment abstraction incorporate the thread-modular approach by letting
tags consider local variables of only one arbitrary single thread, and
only requiring that fragments represent heap cells that can be accessed by
that thread.

\begin{figure}
	\input tsabsshape
\caption{Fragment abstraction}
\label{fig:tsviewshape}
\end{figure} 

Figure~\ref{fig:tsviewshape} shows a set of fragments that is
satisfied wrp.\ to $\thread_2$ by the configuration in Figure~\ref{fig:tsshape}.
There are $7$ fragments, named $\frag_1$, \ldots , $\frag_2$. Two of
these ($\frag_3$ and $\frag_7$) consist of a tag that points to $\dangconst$,
and the other consist of a pair of pointer-connected tags.
Consider the tag which occurs in fragment $\frag_7$; it is the bottom-rightmost
tag. This tag is an abstraction of the
bottom-rightmost heap cell in Figure~\ref{fig:tsshape}, using the same layout
for fields as Figure~\ref{fig:tsshape}. The different non-pointer fields
are represented as follows.
\begin{itemize}
\item The $\tt data$ field of the tag (to the left) abstracts the data value
  $2$ to the set of observer registers with that value: in this case
  $\reg_2$.
\item The $\tt ts$ field (at the top) abstracts the timer value $15$ to
  the possible relations with $\tt ts$-fields of heap cells with the same
  data value as each observer registers. Recall that observer registers
  $\reg_1$ and $\reg_2$ have values $4$ and $2$, respectively. There are
  three heap cells with $\tt data$ field value $4$, all with a $\tt ts$
  value less than $15$. There is one heap cell with
  $\tt data$ field value $2$, having $\tt ts$ value $15$.
  Consequently, the abstraction of the $\tt ts$ field maps $\reg_1$ to
  $\set{>}$ and $\reg_2$ to $\set{=}$: this is shown as the mapping
  $\lambda_4$ in Figure~\ref{fig:tsviewshape}.
\item The $\tt mark$ field assumes values from a small finite domain and
  is represented precisely as in concrete heap cells
\end{itemize}
Above the top, the tag contains the thread-local and global pointer variables
that point to the cell, in this case $\tt youngest$ and $\tt n$.
At the bottom of the tag, the first row contains the global variables 
pointing to cells from which the cell can be reached, in this case
$\tt pools[3]$, as well as observer registers whose value is equal to the
${\tt data}$ field of a cell from which  the cell can be reached, in this
case $\reg_2$ (since the cell itself has the same data value as $\reg_2$).
The second row contains dual information: now for cells that can be reached
from the cell itself (this is again $\reg_2$).

\todo[inline]{Fix the issue with ${\tt pools[3]}$}

Each cell in the heap state of Figure~\ref{fig:tsshape} now satisfies
some tag in Figure~\ref{fig:tsshape}. Moreoever, each pair of pointer-connected
cells (where the pointed-to ``cell'' can also be $\dangconst$)
satisfies some fragment in Figure~\ref{fig:tsshape} in the obvious way.
Conversely, the set of fragments in
Figure~\ref{fig:tsshape} represents the set of heaps in which each pair of
pointer-connected cells satisfies one of its fragments. For instance, the
list pointed to by $\tt pools[3]$ is represented by the sequence of
fragments $\frag_4 \frag_5 \frag_6 \frag_7$.

In order to obtain a complete representation of reachable program configurations,
we must also represent the local states of a thread. This is done in a standard
manner, by applying the same data abstraction as for heap cells. For instance,
the local state of thread $\thread_2$ corresponding to
Figure~\ref{fig:tsshape} is represented by a {\em local symbolic configuration}
that contains the values of the program counter and variable $\tt success$,
abstracts the value of ${\tt k}$ into the set $\set{{\tt me},{\tt ot}}$ and
applies the timestamp abstraction to ${\tt maxTS}$.

We are now ready to present our symbolic representation of a set of
reachable program configurations. 
%% It maps each local symbolic configuration to a set of fragments, which
%% is a symbolic representation of the set of corresponding reachable heaps.
Our symbolic representation is a mapping from a set of local symbolic
configurations, which maps each local symbolic configuration in its domain
to a set of fragments.
A global configuration satisfies a symbolic representation $\symbrep$
if the local state of each thread $\thread$ satisfies some local symbolic
configuration in the domain of $\symbrep$, which is mapped to a set
of fragments, which is satisfied by heap wrp.\ to thread $\thread$.


Our symbolic representation represents the reachable local states of a thread by
a finite set of {\em local symbolic configurations}, using our
data abstraction.
It maps each local symbolic configuration to a set of fragments, which
is a symbolic representation of the set of corresponding reachable heaps.
Thus, our symbolic representation is a mapping from a set of local symbolic
configurations, which maps each local symbolic configuration in its domain
to a set of fragments.
A global configuration satisfies a symbolic representation $\symbrep$
if the local stat of each thread $\thread$ satisfies some local symbolic
configuration in the domain of $\symbrep$, which is mapped to a set
of fragments, which represents the heap that is accessible to $\thread$ in
the global configuration. In the following, we explain more precisely the
local symbolic configurations, and our fragment abstraction.

In the  verification, we must compute a symbolic representation
that is satisfied by all reachable program configurations (recall that
program configurations include the state of the observer).
This invariant is obtained by an abstract-interpretation-based
fixpoint procedure, which starts
from a representation of the set of initial configurations, and
thereafter repeatedly performs
postcondition computations that extend the
symbolic representation by the effect of any execution step of the program,
until convergence.
This procedure is presented in Section~\ref{subsect:postcond}.





