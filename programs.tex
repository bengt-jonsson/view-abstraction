\section{Concurrent Data Structure Algorithms}
\label{programs:section}

In this section, we introduce our representation of
concurrent data structure algorithms, we define the correctness criterion of
linearizability, we introduce observers and how to use them for specifying
linearizability.

\subsection{Concurrent Data Structure Algorithms}

We begin by introducing (sequential) data structures.
A {\it data structure} $\dstruct$ is a pair
$\tuple{\ddomain,\malphabet}$,
where $\ddomain$ is a (possibly infinite) {\it data domain} and 
$\malphabet$ is an alphabet of {\em method names}.
%
%% We assume w.l.o.g. that the infinite data domain $\ddomain$ is given by the set 
%% $\intgrs$ of integers.
%
An {\it operation} 
%\todo{I have changed {\it event} to {\it operation.}}
$\op$ is of the form
$\mname(\indata,\outdata)$ where 
$\mname\in\malphabet$ is a method name and 
$\indata$ are the {\it input} resp.\ {\it output} values, each of which
is either in $\ddomain$ or in some fixed finite domain (such as the set of
booleans).
%
For some method names, the input or output value is absent from the operation.
%% \todo[inline]{Here, we should give an example of a stack trace. And maybe
%% omit the rest of this subsubsection}
For example, for the {\tt Stack} data structure, the method names
are $\tt push$ and $\tt pop$.
%
The operation ${\tt push(3)}$, where $\tt 3$ is an input value, pushes
the value $3$, whereas ${\tt pop(4)}$, where $\tt 4$ is an output value,
pops the value $4$.

A {\it trace} of $\dstruct$ is a sequence of operations.
The standard (sequential) semantics of a data structure $\dstruct$ is provided by
a set $\denotationof\dstruct$ of allowed traces,
called the {\it legal} traces of $\dstruct$.
%
%% We often identify $\dstruct$ with its behavior $\denotationof\dstruct$.

%
%(see Appendix~\ref{dstruct:sepcs:section} for a formal specification
%of $\denotationof{\tt Set}$, $\denotationof{\tt Queue}$, and $\denotationof{\tt Stack}$.)

%% \begingroup
%% \setlength\intextsep{-8pt}
%% \setlength{\columnsep}{4pt}
%% \begin{wrapfigure}{R}{6cm}
%% \center 

A {\em concurrent data structure algorithm} operates on a shared
state consisting of shared global variables and a shared heap.
It assigns, to each method name, a method which performs operations
%% In this section, we introduce programs that consist
%% of arbitrary numbers of concurrently executing threads
%% that access a concurrent data structure.
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
%% Each thread executes a method that performs an operation
on the shared state.
Each data structure algorithm also comes with an initialization method,
named {\tt init}, which initializes its shared state.

%\todo{What is the difference between methods here in 
%Section~\ref{linearizability:section}?}
%
Each method declares local variables
  and a method body.
%
Variables are either pointer variables
(to heap cells)
or data variables, assuming values from $\ddomain$
%% an infinite (ordered) domain,
or from some finite set $\fset$ that includes the Boolean values.
%
%% We assume w.l.o.g. that the infinite set is given by the set 
%% $\intgrs$ of integers.
%
The body is built in the standard way
from atomic commands, using standard control
flow constructs (sequential composition, selection, and loop constructs).
%
%% Each statement is equipped with a unique label.
%
We assume that the set of local variables include the input
parameter of the method
in addition to the program counter {\tt pc}.
%% whose value is the label of the next statement to be executed.
%
Method execution is terminated by executing a {\tt return} command,
which may return a value.
The global variables can be
accessed by all threads, whereas local variables can be accessed only
by the thread which is invoking the corresponding method.
%
We assume that all global variables are pointer variables.

%
Heap cells have a fixed set $\fieldset$ of fields, namely
data fields that assume values in
$\ddomain$ or $\fset$, and possibly lock fields.
Furthermore, each cell has one or several named pointer fields.
For instance, in data structure algorithms based on singly linked lists,
each heap cell has a pointer field named ${\tt next}$, in algorithms
based on skip lists, there is an array of pointer fields named
${\tt next[k]}$ where ${\tt k}$ ranges from $0$ up to the maximum level of the
skip list.
%% heap cells are organized into singly-linked lists.%\todo{Possibly with a 
%cycle?}
%% We use the term $\ddomain$-field for a data field that
%% assumes values in $\ddomain$, and the terms $\fset$-field and lock field
%% with analogous meaning.
Atomic commands include assignments between data variables, 
pointer variables, or fields of cells pointed to by a pointer variable.
%
The command ${\tt new Node()}$ allocates a new structure of type
${\tt Node}$ on the heap, and returns a reference to it.
%
The compare-and-swap command ${\tt CAS(\&a,b,c)}$ atomically
compares the values of ${\tt a}$ and ${\tt b}$.
If  equal, it assigns the value of
${\tt c}$ to ${\tt a}$  and returns ${\tt true}$, 
otherwise, it leaves ${\tt a}$ unchanged and returns ${\tt false}$. 
%
We assume a memory management mechanism, which automatically collects
garbage, and ensures that a new cell is fresh, i.e., has
not been used before.
%% this avoids the so-called ABA problem (e.g.,~\cite{MS:QueueAlgorithms}).

We define a {\em program} (over a concurrent data structure) to consist 
of an arbitrary number of concurrently executing threads, each of which
executes a method that performs an operation
on the data structure. We assume that the data structure has been initialized
by the ${\tt init}$ method prior to the start of program execution.

\subsection{Linearizability}
In a concurrent
data structure algorithm, we represent the calling of a method by
a {\it call action} $\call\eid\mname\indata$, and the return of a method by
%% an operation $\mname(\indata,\outdata)$
%% corresponds to two {\it actions}, namely
a {\it return action} $\return\eid\mname\outdata$, where $\eid\in\nat$ is an
{\it action identifier}, which links the call and return of each method invocation.
%
A {\it history} $\history$ is a sequence of actions such that
\begin{inparaenum}[(i)]
\item
  different occurrences of return actions have different action identifiers, and
\item
  for each return action $\action_2$ in $\history$ there is a unique
{\it matching}  call action $a_1$ with the same action identifier and method name, which occurs before $\action_2$ in $\history$.
\end{inparaenum}
%
%% A call action carries always the same method name as its matching return
%% action.
A call action which does not match any return action in $\history$ is said
to be {\em pending}.
A history without pending call actions is said to be {\em complete}.
A {\em completed extension} of $\history$ is a complete history
$\history'$ obtained from $\history$ by
  appending (at the end) zero or more return actions that are matched by
  pending call actions in $\history$, and
  thereafter removing pending call actions.
%
For action identifiers $\eid_1,\eid_2$, we write
$\eid_1\occursbefore\history\eid_2$ to denote that
the return action with identifier $\eid_1$ occurs before
the call action with identifier $\eid_2$ in $\history$.
A history is {\it sequential} if it is of the form
$\action_1\action'_1\action_2\action'_2\cdots\action_\nn\action'_\nn$
%% where $\action'_\nn$ can be absent and
where $\action'_\ii$ is the matching action of $\action_\ii$ 
for all $\ii:1\leq\ii\leq\nn$, i.e., each call action 
is immediately followed by the matching return action. 
%
We identify a sequential history of the above form with
the corresponding trace 
$\op_1\op_2\cdots\op_\nn$ where
$\op_\ii=\mname(\indata_\ii,\outdata_\ii)$,
$\action_\ii=\call{\eid_\ii}{\mname}{\pindata\ii}$, and
$\action_\ii=\return{\eid_\ii}{\mname}{\poutdata\ii}$,
i.e., we merge each call action together with the matching return action
into one operation.
%
A complete history $\history'$ 
is a {\it linearization} of $\history$ if
(i) $\history'$ is a permutation of $\history$,
(ii) $\history'$ is sequential, 
and
(iii) $\eid_1\occursbefore{\history'}\eid_2$
if $\eid_1\occursbefore{\history}\eid_2$
for each pair of action identifiers $\eid_1$ and $\eid_2$.
%
A sequential history $\history'$ is {\it valid} wrt.\ $\dstruct$ if
the corresponding trace is in $\denotationof{\dstruct}$.
%
We say that $\history$ is {\it linearizable} wrt.\ $\dstruct$ if there is
a completed extension of $\history$, which has
a linearization that is valid wrt.\ $\dstruct$.
%
%% A set $\histories$ of histories is {\it linearizable} wrt.\ $\dstruct$ if 
%% all members of $\histories$ are  {\it linearizable} wrt.\ $\dstruct$.
We say that a program $\prog$ is linearizable wrt.\ 
$\dstruct$, in each possible execution, the sequence of call and return actions
is {\em linearizable} wrt.\ $\dstruct$.

\todo[inline]{Should we mention LP method:
The most common technique is to annotate methods with LPs. More precisely,
each method is instrumented to announce precisely when the linearization point
occurs during each method invocation. In many cases, the linearization point
can be associated with a particular statement in the method code (so called
{\em fixed linearization points}). Linearizability can then be verified by
checking that the occurrence of linearization points is allowed by the
sequential semantics of the particular data structure that is implemented.}

%% \begin{figure}
%% \center 

%% \begin{tikzpicture}[]

%% \node(s0)[draw,line width=0.5pt,circle,text=black]
%% {$s_0$};

%% \node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(80pt,0pt)$) {$s_1$};


%% \node(s2)[draw,line width=0.5pt,circle,text=black,scale=0.9,anchor=center ]
%% at ($(s0.center)+(40pt,-61pt)$) {$s_2$};

%% \node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
%% at ($(s0.center)+(40pt,-61pt)$) {};


%% \draw[->,>=stealth,out=45,in=135,line width=0.5pt] (s0) to 
%% node[above=-1pt,pos=0.5] {\footnotesize${\tt add}(x,\true)$} 
%% (s1);

%% \draw[->,>=stealth,out=225,in=315,line width=0.5pt] (s1) to node[above=0pt,pos=0.5] {\footnotesize${\tt rmv}(x,\true)$} 
%% (s0);

%% \draw [->,>=stealth,thick,out=270,in=180,line width=0.5pt] (s0) 
%% to 
%% node[left,pos=0.2] {\footnotesize${\tt add}(x,\false)$} 
%% node[left,pos=0.45] {\footnotesize${\tt rmv}(x,\true)$} 
%% node[left,pos=0.8] {\footnotesize${\tt ctn}(x,\true)$} 
%%  (s2);

%% \draw [->,>=stealth,thick,out=270,in=0,line width=0.5pt] (s1) 
%% to 
%% node[right,pos=0.2] {\footnotesize${\tt add}(x,\true)$} 
%% node[right,pos=0.45] {\footnotesize${\tt rmv}(x,\false)$} 
%% node[right,pos=0.8] {\footnotesize${\tt ctn}(x,\false)$} 
%%  (s2);




%% \end{tikzpicture}
%% \caption{Set observer.}
%% \label{set:observer:fig}
%% \end{figure}

\subsection{Specification by Observers}
To verify correctness of a data structure algorithm,
we must verify that any history, i.e., sequence of call and return actions,
of any program execution satisfies the linearizability criterion.
For this, we employ observers, using two different methods.
For concurrent stacks and queues, we use the technique illustrated in
Section~\ref{sec:overview}, in which observers check the sequence of
method call and return actions.
It was recently established~\cite{BEEH:icalp15,HSV:concur13} that this
technique can precisely characterize linearizability for stacks and queues
(under a very mild technical condition of data-independence).
For concurrent sets, we use the obsevers to check the sequence of
linerarization points of methods. This puts a mild burden on the user to
annotate the methods with linearization points, in some cases
combined with a linearization policy~\cite{Quy:sas16}.
For completeness, we here describe both approaches.

\subsubsection{Observers for Call an Return Actions}
For stacks and queues, it was recently established
~\cite{BEEH:icalp15,HSV:concur13} that the linearizability criterion is equivalent to
a small number of simple ordering constraints of the following form
\begin{quote}
for one or two {\em arbitrary} data values, the subsequence of
call and return actions with these data values as parameters is in 
a particular regular set.
\end{quote}
The complement of each such constraint can be
expressed by an {\em observer}, as introduced in~\cite{AHHR:integrated}. 
%
Observers are
finite automata extended with a finite set of {\em registers}
that assume values in $\ddomain$. 
%
%
At initialization,
the registers are nondeterministically
assigned arbitrary distinct values, which never change
during a run of the observer. 
%
Formally, an observer $\observer$ is a tuple
$\otuple$ where $\ostateset$ is a finite set 
of {\it observer states} including the 
{\it initial state} $\oinitstate$ and
the {\it accepting state} $\oaccstate$, 
a finite set $\ovarset$  
of {\it registers}, and $\otransitionset$ is a finite
set of {\it transitions}.
%
%
Transitions are of the form 
$\tuple{\ostate_1,\calltwo{\mname}{\inxvar,\outxvar},\ostate_2}$ or
$\tuple{\ostate_1,\returntwo{\mname}{\inxvar,\outxvar},\ostate_2}$
where $\inxvar$ and $\outxvar$ are either registers, or constants, or absent,
i.e., transitions are labeled by 
operations whose input or output data values may be parameterized on registers.
The observer processes a history one action at a time.
%
If there is a transition, whose label (after replacing registers by their
values) matches the action, such a transition is performed. 
%
If there is no
such transition, the observer remains in its current state.
The observer accepts a history if it can  be processed in such a way that
an accepting state is reached.
%
The observer is defined in such a way that it accepts precisely those
histories that do {\em not} satisfy the constraint represented by the observerer.
In Figures~\ref{fig:nonpopobserver:fig} and~\ref{fig:lifostack:fig} we showed
observers for two constraints in the specification of linearizability for
concurrent stack algorithms.
%
%Other
%observers for stacks and queues can be found in~\cite{AHHR:integrated}
%and Appendix~\ref{dstruct:sepcs:section}.
\subsubsection{Observers for Linearization Points}
In this technique, we use observers to specify that a trace (i.e., 
sequence of operations)  is allowed by the sequential data structure.
\begin{figure}{R}{6cm}
\center 

\begin{tikzpicture}[]

\node(s0)[draw,line width=1pt,fill=blue!10,circle,text=black]
{$s_0$};

\node(s1)[draw,line width=1pt,fill=blue!10,circle,text=black,anchor=center]
at ($(s0.center)+(80pt,0pt)$) {$s_1$};


\node(s2)[draw,line width=1pt,fill=blue!10,circle,text=black,scale=0.9,anchor=center ]
at ($(s0.center)+(40pt,-61pt)$) {$s_2$};

\node[draw,line width=1pt,circle,text=black,anchor=center,minimum size=5mm]
at ($(s0.center)+(40pt,-61pt)$) {};


\draw[->,>=stealth,out=45,in=135,line width=1pt] (s0) to 
node[above=-1pt,pos=0.5] {\footnotesize${\tt add}(x,\true)$} 
(s1);

\draw[->,>=stealth,out=225,in=315,line width=1pt] (s1) to node[above=0pt,pos=0.5] {\footnotesize${\tt rmv}(x,\true)$} 
(s0);

\draw [->,>=stealth,thick,out=270,in=180,line width=1pt] (s0) 
to 
node[left,pos=0.2] {\footnotesize${\tt add}(x,\false)$} 
node[left,pos=0.45] {\footnotesize${\tt rmv}(x,\true)$} 
node[left,pos=0.8] {\footnotesize${\tt ctn}(x,\true)$} 
 (s2);

\draw [->,>=stealth,thick,out=270,in=0,line width=1pt] (s1) 
to 
node[right,pos=0.2] {\footnotesize${\tt add}(x,\true)$} 
node[right,pos=0.45] {\footnotesize${\tt rmv}(x,\false)$} 
node[right,pos=0.8] {\footnotesize${\tt ctn}(x,\false)$} 
 (s2);




\end{tikzpicture}
\caption{Set observer.}
\label{set:observer:fig}
\end{figure}
As an example, Figure~\ref{set:observer:fig}
depicts an observer that accepts the
sequences of operations that are {\em not} in $\denotationof{\tt Set}$.
This technique presupposes to
instrument each method to generate operations at its linearization point (LP),
using standard techniques.
For some algorithms, LPs can be statically affixed to method statements.
For algorithms where this is not possible, we use the extension of this
technique by linearization policies of~\cite{Quy:sas16}.


\todo[inline]{It remains to take care of the criterion that the
  call of pop already has a return value}

\subsection{Formal Semantics}
\label{semantics:section}
For completeness, we formalize the semantics of programs and
observers. Below, we assume a program $\prog$  with a set 
$\glvarset$ of global variables, and an ordering constraint
specified by an observer $\observer$.
%
%% We assume that each cell contains a set $\fieldset$ of fields
%% where each field is either a lock or a data field.
We assume here, to simplify notation,
that each thread $\thread$ executes one method
denoted $\methodof\thread$.
%% First, we define the state of the heap and the transition
%% relation induced by a single thread.
%% %
%% From this we derive the semantics of the program and use it to
%% define its history set.
%% %
%% We then define the semantics of observers.
%% Finally, we define the product of the augmented program and the observer.
%% This product will ensure that observer
%% On this basis, we can formally state and prove the correctness of
%% our approach, as Theorem~\ref{thm:soundness}.
%% Finally, we introduce  the soundness theorem 
%% which states that linearizability of the original program
%% is implied by the fact that the monitor never enters
%% its ``error'' state.
For a function $\fun:A\mapsto B$ from a set $A$ to
a set $B$, we use 
$\fun[\aelem_1\assigned\belem_1,\ldots,\aelem_\nn\assigned\belem_\nn]$ 
to denote the function
$\fun'$ such that $\fun'(\aelem_\ii)=\belem_\ii$ and 
$\fun'(\aelem)=\fun(\aelem)$ if 
$\aelem\not\in\set{\aelem_1,\ldots,\aelem_\nn}$.
%

%



\paragraph{Heaps.}
A {\it heap (state)} is a tuple $\heap=\heaptuple$, where
(i)
$\cellset$ is a finite set of cells, including the two special cells
$\nullconst$ and $\dangconst$ (dangling);
%
we define $\mcellset=\cellset\setminus\set{\nullconst,\dangconst}$,
%
(ii)
for each pointer field $\pfield_i$ there is a total function
$\pfield_i:\mcellset\to\cellset$
that defines where that pointer field points,
%% the next-pointer relation, i.e., $\priflesuccof{\cell}=\cell'$ means that cell $\cell$ points to $\cell'$,
(iii)
$\glval:\glvarset\to\cellset$ maps the global (pointer)
variables to their values, and
(iv)
$\cellval:\cellset\times\fieldset\to\fset\cup\ddomain$
maps data and lock fields of each cell to their values.
%
We let $\initheap$ denote the initial heap produced by the
{\tt init} method.

\paragraph{Threads.}
A {\it local state} $\lstate$ of a thread $\thread$
wrt.\ a heap $\heap$ defines 
the values of its local variables, including the program counter
{\tt pc} and the input parameter for the method executed by $\thread$.
In addition, there is the special initial state $\idlestate$,
and terminated state $\terminatedstate$.
%
%% A {\em projection} $\proj$ is a pair
%% $\tuple{\lstate,\heap}$, where $\lstate$
%% is a local state wrt.\  the heap $\heap$.
%
We formalize the behavior of a thread $\thread$ by
a labeled transition relation $\movesto\thread{}$
on pairs $\tuple{\lstate,\heap}$ consisting of a local state
$\lstate$ and a heap $\heap$, with three types of transitions:
%
\begin{enumerate}
\item
$\tuple{\idlestate,\heap}
\movesto\thread{{\call\thread\mname\indata}}
\tuple{\initlstateof\thread,\heap}$ activates
thread $\thread$ through a transition
labeled by a call action with $\thread$ as the action identifier and
$\mname=\methodof\thread$, taking $\thread$
to an initial local state $\initlstateof\thread$
where $\indata$ is the value of its input parameter,
the value of {\tt pc} is the label of the first statement of the method, and
the other local variables are undefined.
\item
$\tuple{\lstate,\heap}\movesto\thread{\mathit{label}}\tuple{\lstate',\heap'}$
  denotes execution of method statements, where the label $\mathit{label}$ is
  empty for most statements, except for those that are instrumented
  with a linearization point (provided we use the method where
  observers check sequences of operations),
  in which case $\mathit{label}$ is the corresponding operation.
%
%% We write $\proj\movesto\thread\lbl\proj'$ 
%% to denote that the statement labeled by $\lbl$ can be executed
%% from $\proj$, yielding $\proj'$.
%% Note that the next move of $\thread$ is uniquely determined
%% by $\proj$, since $\thread$ cannot access
%% the local states of other threads.
%
\item 
$\tuple{\lstate,\heap}\movesto\thread{\return\thread\mname\outdata}
\tuple{\terminatedstate,\heap}$ terminates
thread $\thread$ through execution of its {\tt return} command, labeled
by a return action with $\thread$ as action identifier,
$\mname=\methodof\thread$, and $\outdata$
as the returned value.
\end{enumerate}


\paragraph{Programs.}
A {\it configuration} of a program
$\prog$ is a tuple
$\tuple{\athreads,\lstatemapping,\heap}$ where
$\athreads$ is a set of threads,
$\heap$ is a heap, and
$\lstatemapping$
%% is a {\it local state mapping} over $\athreads$ wrt.\ $\heap$ that
maps each thread $\thread\in\athreads$ to its
local state $\lstatemappingof\thread$ wrt.\ $\heap$.
%
%% We use $\confsetof\prog$ to denote the set of configurations
%% of $\prog$.
%
The initial configuration
%% $\initconfof\prog\in\confsetof\prog$ 
$\initconfof\prog$ 
is the pair
$\tuple{\initlstatemapping,\initheap}$,
where
%% $\initheap$ is the initial heap, and
$\initlstatemappingof\thread=\idlestate$ for each
$\thread\in\athreads$,
%% i.e.,
%% $\prog$ starts its execution from a configuration with an initial
%% heap, and with each thread in its initial local state.
%
A program $\prog$ induces a transition relation
$\movesto\prog{}$ where each step corresponds to one move of a single thread.
I.e., 
there is a transition of form
$\tuple{\athreads,\lstatemapping,\heap}
\movesto\prog{\mathit{label}}
\tuple{\athreads,\lstatemapping[\thread\leftarrow\lstate'],\heap'}$
whenever the transition relation $\movesto\thread{}$ of thread $\thread$
has a transition
$\tuple{\lstate,\heap}\movesto\thread{\mathit{label}}\tuple{\lstate',\heap'}$,
%% where the label $\lbl$ is either a call or return action or the empty label.
%
%% Note that the only visible transitions are those
%% corresponding to call and return actions.

\paragraph{Cross-Product of Program and Observer}
We use $\system=\prog\compose\observer$ to denote the augmented program obtained
executing running $\prog$ and $\observer$ together.
%
The initial configuration of $\system$ is
$\tuple{\initconfof\prog,\oinitstate}$.
Transitions of $\system$ are of the form
$\tuple{\pconf,\ostate}\movesto{\system}{}\tuple{{\pconf}',\ostate'}$,
obtained from a transition
$\pconf \movesto\prog{l} {\pconf}'$ of the program with some (possibly empty)
label $\mathit{label}$, and
a corresponding synchronizing transition
$\tuple{\ostate,\mathit{label},\ostate'}$
of the observer
(which is the trivial self-loop $\tuple{\ostate,\mathit{\emptyword},\ostate}$
if the observer does not have a synchronizing transition).
The verification problem is now to check that $\system$ cannot reach
a configuration $\tuple{\pconf,\oaccstate}$ with an accepting
observer state.

%% A {\it history} of $\prog$ is the sequence of call and return actions in
%% some sequence of transitions of $\movesto\prog{}$ from the initial configuration.
%% We define $\historyof\prog$ to be the set of histories of $\prog$.









