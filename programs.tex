\section{Concurrent Data Structure Implementations}
\label{programs:section}
In this section, we introduce our representation of
concurrent data structure implementations, we define the correctness criterion of
linearizability, we introduce observers and how to use them for specifying
linearizability.

\subsection{Concurrent Data Structure Implementations}

We first introduce (sequential) data structures.
A {\it data structure} $\dstruct$ is a pair
$\tuple{\ddomain,\malphabet}$,
where $\ddomain$ is a (possibly infinite) {\it data domain} and 
$\malphabet$ is an alphabet of {\em method names}.
%
%% We assume w.l.o.g. that the infinite data domain $\ddomain$ is given by the set 
%% $\intgrs$ of integers.
%
An {\it operation} 
%\todo{I have changed {\it event} to {\it operation.}}
$\op$ is of the form
$\mname(\indata,\outdata)$ where 
$\mname\in\malphabet$ is a method name and 
$\indata$ are the {\it input} resp.\ {\it output} values, each of which
is either in $\ddomain$ or in some fixed finite domain (such as the booleans).
%
For some method names, the input or output value is absent from the operation.
A {\it trace} of $\dstruct$ is a sequence of operations.
The (sequential) semantics of a data structure $\dstruct$ is given by
a set $\denotationof\dstruct$ of allowed traces.
%% called the {\it legal} traces of $\dstruct$.

\todo[inline]{Quy: replace the below by an example of a set trace.}

For example, for the {\tt Stack} data structure, the method names
are $\tt push$ and $\tt pop$.
%
The operation ${\tt push(3)}$, where $\tt 3$ is an input value, pushes
the value $3$, whereas ${\tt pop(4)}$, where $\tt 4$ is an output value,
pops the value $4$.

%
%% We often identify $\dstruct$ with its behavior $\denotationof\dstruct$.

%
%(see Appendix~\ref{dstruct:sepcs:section} for a formal specification
%of $\denotationof{\tt Set}$, $\denotationof{\tt Queue}$, and $\denotationof{\tt Stack}$.)

%% \begingroup
%% \setlength\intextsep{-8pt}
%% \setlength{\columnsep}{4pt}
%% \begin{wrapfigure}{R}{6cm}
%% \center 

A {\em concurrent data structure implementation} operates on a shared
state consisting of shared global variables and a shared heap.
It assigns, to each method name, a method which performs operations
%% In this section, we introduce programs that consist
%% of arbitrary numbers of concurrently executing threads
%% that access a concurrent data structure.
%(Appendix~\ref{planguage:section} contains
%the language syntax used in our tool).
%
%% Each thread executes a method that performs an operation
on the shared state.
Each data structure implementation also comes with an initialization method,
named {\tt init}, which initializes its shared state.

We assume that all global variables are pointer variables.
Heap cells have a fixed set $\fieldset$ of fields, namely
data fields that assume values in
$\ddomain$ or $\fset$, and possibly lock fields.
We use the term $\ddomain$-field for a data field that
assumes values in $\ddomain$, and the terms $\fset$-field and lock field
with analogous meaning.
Furthermore, each cell has one or several named pointer fields.
For instance, in data structure implementations based on singly linked lists
each heap cell has a pointer field named {\tt next}; in implementations
based on skiplists there is an array of pointer fields named
{\tt next[k]} where {\tt k} ranges from $1$ to the maximum level of the
skiplist.
%% heap cells are organized into singly-linked lists.%\todo{Possibly with a 
%cycle?}

%\todo{What is the difference between methods here in 
%Section~\ref{linearizability:section}?}
%
Each method declares local variables and a method body.
%
The set of local variables include the input parameter of the method and
the program counter {\tt pc}.
The global variables can be
accessed by all threads, whereas local variables can be accessed only
by the thread which is invoking the corresponding method.
Variables are either pointer variables (to heap cells) or data variables, assuming values from $\ddomain$ or from some finite set $\fset$ that includes the boolean values.
%
%
%% We assume w.l.o.g. that the infinite set is given by the set 
%% $\intgrs$ of integers.
%
The body is built in the standard way
from atomic commands, using standard control
flow constructs (sequential composition, selection, and loop constructs).
%
%% Each statement is equipped with a unique label.
%
%% whose value is the label of the next statement to be executed.
%
Method execution is terminated by executing a {\tt return} command,
which may return a value.
%
Atomic commands include assignments between data variables, 
pointer variables, or fields of cells pointed to by a pointer variable.
%
The command {\tt new Node()} allocates a new structure of type
{\tt Node} on the heap, and returns a reference to it.
%
The compare-and-swap command {\tt CAS(\&a,b,c)} atomically
compares the values of {\tt a} and {\tt b}.
If  equal, it assigns the value of
{\tt c} to {\tt a}  and returns {\tt true}, 
otherwise, it leaves {\tt a} unchanged and returns {\tt false}. 
\bjcom{Quy: You may want instead to describe Compare-and-set}
%
We assume a memory management mechanism, which automatically collects
garbage, and ensures that a new cell is fresh, i.e., has
not been used before; this avoids the so-called
ABA problem (e.g.,~\cite{MS:QueueAlgorithms}).

We define a {\em program} (over a concurrent data structure) to consist 
of an arbitrary number of concurrently executing threads, each of which
executes a method that performs an operation on the data structure.
We assume concurrent execution according to sequentially consistent memory
model.
We assume that the data structure has been initialized
by the {\tt init} method prior to the start of program execution.

\subsection{Linearizability}
In a concurrent
data structure implementation, we represent the calling of a method by
a {\it call action} $\call\eid\mname\indata$, and the return of a method by
%% an operation $\mname(\indata,\outdata)$
%% corresponds to two {\it actions}, namely
a {\it return action} $\return\eid\mname\outdata$, where $\eid\in\nat$ is an
{\it action identifier}, which links the call and return of each method invocation.
%
A {\it history} $\history$ is a sequence of actions such that
\begin{inparaenum}[(i)]
\item
  different occurrences of return actions have different action identifiers, and
\item
  for each return action $\action_2$ in $\history$ there is a unique
{\it matching}  call action $a_1$ with the same action identifier and method name, which occurs before $\action_2$ in $\history$.
\end{inparaenum}
%
%% A call action carries always the same method name as its matching return
%% action.
A call action which does not match any return action in $\history$ is said
to be {\em pending}.
A history without pending call actions is said to be {\em complete}.
A {\em completed extension} of $\history$ is a complete history
$\history'$ obtained from $\history$ by
  appending (at the end) zero or more return actions that are matched by
  pending call actions in $\history$, and
  thereafter removing the call actions that are still pending.
%
For action identifiers $\eid_1,\eid_2$, we write
$\eid_1\occursbefore\history\eid_2$ to denote that
the return action with identifier $\eid_1$ occurs before
the call action with identifier $\eid_2$ in $\history$.
A complete history is {\it sequential} if it is of the form
$\action_1\action'_1\action_2\action'_2\cdots\action_\nn\action'_\nn$
%% where $\action'_\nn$ can be absent and
where $\action'_\ii$ is the matching action of $\action_\ii$ 
for all $\ii:1\leq\ii\leq\nn$, i.e., each call action 
is immediately followed by the matching return action. 
%
We identify a sequential history of the above form with
the corresponding trace 
$\op_1\op_2\cdots\op_\nn$ where
$\op_\ii=\mname(\indata_\ii,\outdata_\ii)$,
$\action_\ii=\call{\eid_\ii}{\mname}{\pindata\ii}$, and
$\action_\ii=\return{\eid_\ii}{\mname}{\poutdata\ii}$,
i.e., we merge each call action together with the matching return action
into one operation.
%
A complete history $\history'$ 
is a {\it linearization} of $\history$ if
(i) $\history'$ is a permutation of $\history$,
(ii) $\history'$ is sequential, 
and
(iii) $\eid_1\occursbefore{\history'}\eid_2$
if $\eid_1\occursbefore{\history}\eid_2$
for each pair of action identifiers $\eid_1$ and $\eid_2$.
%
A sequential history $\history'$ is {\it valid} wrt.\ $\dstruct$ if
the corresponding trace is in $\denotationof{\dstruct}$.
%
We say that $\history$ is {\it linearizable} wrt.\ $\dstruct$ if there is
a completed extension of $\history$, which has
a linearization that is valid wrt.\ $\dstruct$.
%
%% A set $\histories$ of histories is {\it linearizable} wrt.\ $\dstruct$ if 
%% all members of $\histories$ are  {\it linearizable} wrt.\ $\dstruct$.
We say that a program $\prog$ is linearizable wrt.\ 
$\dstruct$ if, in each possible execution, the sequence
of call and return actions is {\em linearizable} wrt.\ $\dstruct$.

%% \begin{figure}
%% \center 

%% \begin{tikzpicture}[]

%% \node(s0)[draw,line width=0.5pt,circle,text=black]
%% {$s_0$};

%% \node(s1)[draw,line width=0.5pt,circle,text=black,anchor=center]
%% at ($(s0.center)+(80pt,0pt)$) {$s_1$};


%% \node(s2)[draw,line width=0.5pt,circle,text=black,scale=0.9,anchor=center ]
%% at ($(s0.center)+(40pt,-61pt)$) {$s_2$};

%% \node[draw,line width=0.5pt,circle,text=black,anchor=center,minimum size=5mm]
%% at ($(s0.center)+(40pt,-61pt)$) {};


%% \draw[->,>=stealth,out=45,in=135,line width=0.5pt] (s0) to 
%% node[above=-1pt,pos=0.5] {\footnotesize${\tt add}(x,\true)$} 
%% (s1);

%% \draw[->,>=stealth,out=225,in=315,line width=0.5pt] (s1) to node[above=0pt,pos=0.5] {\footnotesize${\tt rmv}(x,\true)$} 
%% (s0);

%% \draw [->,>=stealth,thick,out=270,in=180,line width=0.5pt] (s0) 
%% to 
%% node[left,pos=0.2] {\footnotesize${\tt add}(x,\false)$} 
%% node[left,pos=0.45] {\footnotesize${\tt rmv}(x,\true)$} 
%% node[left,pos=0.8] {\footnotesize${\tt ctn}(x,\true)$} 
%%  (s2);

%% \draw [->,>=stealth,thick,out=270,in=0,line width=0.5pt] (s1) 
%% to 
%% node[right,pos=0.2] {\footnotesize${\tt add}(x,\true)$} 
%% node[right,pos=0.45] {\footnotesize${\tt rmv}(x,\false)$} 
%% node[right,pos=0.8] {\footnotesize${\tt ctn}(x,\false)$} 
%%  (s2);




%% \end{tikzpicture}
%% \caption{Set observer.}
%% \label{set:observer:fig}
%% \end{figure}

\subsection{Specification by Observers}
%% \todo[inline]{Should we mention LP method:
%% The most common technique is to annotate methods with LPs. More precisely,
%% each method is instrumented to announce precisely when the linearization point
%% occurs during each method invocation. In many cases, the linearization point
%% can be associated with a particular statement in the method code (so called
%% {\em fixed linearization points}). Linearizability can then be verified by
%% checking that the occurrence of linearization points is allowed by the
%% sequential semantics of the particular data structure that is implemented.}

To verify correctness of a data structure implementation,
we must verify that any history, i.e., sequence of call and return actions,
of any program execution satisfies the linearizability criterion.
This is equivalent to
requiring that each operation on the data structure can be considered as being
performed atomically at some point (called the {\em linearization point (LP)})
between its invocation and return.
To check linearizability of set implementations, the user
first instruments each method so that it generates
a corresponding operation precisely when the method executes its LP, using
the technique of linearization policies~\cite{Quy:sas16}. 
In the case of the skiplist algorithm, this instrumentation simply consists
of associating LPs to fixed statements in the code.
\bjcom{Quy: Please illustrate here how this is done for the skiplist}
Thereafter, it should be checked that the concurrent execution of any
program generates (through its instrumentation) a sequence of operations which satisfies the semantics of the data structure. 
This check is performed by an {\em observer}, which 
monitors the sequence of operations that is announced by the
instrumentation, and report when it violates the semantics of the set
data structure.

%% {\em Observers} are
%% finite automata extended with a finite set of {\em registers}
%% that assume values in $\intgrs$, which are 
%% nondeterministically initialized with arbitrary values, which never change
%% during a run of the observer. 
%% The observer accepts a trace if, for {\em some} initial values of the
%% registers, the trace can  be processed in such a way that
%% an accepting state is reached.
%% In other words, the observer is defined in such a way that it accepts precisely those
%% traces that do {\em not} belong to the behavior
%% of the data structure.
%% %% Observers can be used to give {\it exact} specifications of
%% %% the behaviors of data structures such as sets, queues, and stacks.
%% %
%% %
%% Fig.~\ref{set:observer:fig}
%% depicts an observer that accepts the
%% sequences of operations that do {\em not} conform to the semantics of a set
%% data structure.



%% For queue and stack implementations, we employ another technique which does
%% not require user annotation.
%% For stacks and queues, it was recently established
%% ~\cite{BEEH:icalp15,HSV:concur13} that the linearizability criterion can be
%% checked by an observer

%% is equivalent to
%% a small number of simple ordering constraints of the following form
%% \begin{quote}
%% for one or two {\em arbitrary} data values, the subsequence of
%% call and return actions with these data values as parameters is in 
%% a particular regular set.
%% \end{quote}
%% The complement of each such constraint can be

%% The user then instruments
%% these statements so that they also announce the corresponding operation on
%% the data structure. For instance, \bjcom{Quy: Please insert a text of
%% how some LP is inserted into the code of Figure~\ref{sl-code:fig}}.
%% Having instrumented methods at LPs, we
%% then consider an arbitrary concurrent program consisting of
%% an arbitrary collection of threads, each of which executes some method call.
%% We must check that the concurrent execution of such a program generates
%% (through its instrumentation) a sequence of operations which satisfies the
%% semantics of the set data structure. 
%% This check is performed by an {\em observer}, which 
%% monitors the sequence of operations that is announced by the
%% instrumentation, and reports when it violates the semantics of the set
%% data structure.

Formally, an observer $\observer$ is a tuple
$\otuple$ where $\ostateset$ is a finite set 
of {\it observer states} including the 
{\it initial state} $\oinitstate$ and
the {\it accepting state} $\oaccstate$, 
a finite set $\ovarset$  
of {\it registers}, and $\otransitionset$ is a finite
set of {\it transitions}.
%
%
Transitions are of the form 
$\tuple{\ostate_1,\mname(\indata,\outdata),\ostate_2}$,
$\mname\in\malphabet$ is a method name and 
$\inxvar$ and $\outxvar$ are either registers or constants, i.e.,
transitions are labeled by 
operations whose input or output data may be parameterized on registers.
The observer processes a sequence of operations one operation at a time.
%
If there is a transition, whose label (after replacing registers by their
values) matches the operation, such a transition is performed. 
%
If there is no
such transition, the observer remains in its current state.
The observer accepts a sequence if it can  be processed in such a way that
an accepting state is reached.
%
The observer is defined in such a way that it accepts precisely those
sequences that are {\em not} in $\denotationof{\dstruct}$.
Fig.~\ref{set:observer:fig}
depicts an observer for the set data structure.

For stacks and queues, it was recently established
~\cite{BEEH:icalp15,HSV:concur13} that the set of linearizable histories,
i.e., sequences of call and return actions,
can be exactly specified by an observer.
Thus, we check linearizability for queue and stack implementations without
any user-supplied instrumentation, by using an observer which accepts
precisely the complement of the set of linearizable histories.
We illustrate this in Section XXX.


%% a small number of simple ordering constraints of the following form
%% \begin{quote}
%% for one or two {\em arbitrary} data values, the subsequence of
%% call and return actions with these data values as parameters is in 
%% a particular regular set.
%% \end{quote}
%% The complement of each such constraint can be
%% expressed by an {\em observer}, as introduced in~\cite{AHHR:integrated}. 
%% %
%% Observers are
%% finite automata extended with a finite set of {\em registers}
%% that assume values in $\ddomain$. 
%% %
%% %
%% At initialization,
%% the registers are nondeterministically
%% assigned arbitrary distinct values, which never change
%% during a run of the observer. 
%% %
%% \bjcom{In Figure XXX we showed observers for the stack}
%% %
%Other
%observers for stacks and queues can be found in~\cite{AHHR:integrated}
%and Appendix~\ref{dstruct:sepcs:section}.

\todo[inline]{It remains to take care of the criterion that the
  call of pop already has a return value}

\subsection{Some Notation}
\label{semantics:section}
We conclude this section by introducing some terminology and
notation that will be use in subsequent sections.
We assume a program $\prog$
%% with a set $\glvarset$ of global variables,
and a specification given by an observer $\observer$.
%
%% We assume that each cell contains a set $\fieldset$ of fields
%% where each field is either a lock or a data field.
We assume that each thread $\thread$ executes one method
denoted $\methodof\thread$.
%% First, we define the state of the heap and the transition
%% relation induced by a single thread.
%% %
%% From this we derive the semantics of the program and use it to
%% define its history set.
%% %
%% We then define the semantics of observers.
%% Finally, we define the product of the augmented program and the observer.
%% This product will ensure that observer
%% On this basis, we can formally state and prove the correctness of
%% our approach, as Theorem~\ref{thm:soundness}.
%% Finally, we introduce  the soundness theorem 
%% which states that linearizability of the original program
%% is implied by the fact that the monitor never enters
%% its ``error'' state.

%% For a function $\fun:A\mapsto B$ from a set $A$ to
%% a set $B$, we use 
%% $\fun[\aelem_1\assigned\belem_1,\ldots,\aelem_\nn\assigned\belem_\nn]$ 
%% to denote the function
%% $\fun'$ such that $\fun'(\aelem_\ii)=\belem_\ii$ and 
%% $\fun'(\aelem)=\fun(\aelem)$ if 
%% $\aelem\not\in\set{\aelem_1,\ldots,\aelem_\nn}$.
%

%

\paragraph{Heaps.}
A {\it heap (state)} consists of a finite set
$\cellset$ of cells, including the two special cells
$\nullconst$ and $\dangconst$ (dangling).
We represent each named field of the cell type by a function from the set of
nontrivial cells $\mcellset \cellset\setminus\set{\nullconst,\dangconst}$ to
the appropriate domain, e.g., $\cellset$ in the case of pointer fields.
Thus, the values of {\tt next}-fields is represented by a function
${\tt next}: \mcellset\mapsto\cellset$.

\paragraph{Threads.}
A {\it local state} $\lstate$ of a thread $\thread$
%% wrt.\ a heap $\heap$
defines  the values of its local variables, including the program counter
{\tt pc} and the input parameter for the method executed by $\thread$.
The behavior of a thread $\thread$ can be formalized as 
a labeled transition relation $\movesto\thread{}$
on pairs $\tuple{\lstate,\heap}$ consisting of a local state
$\lstate$ and a heap $\heap$. Transitions are normally unlabeled, except
%% with transitions of the form:
%% %% $\tuple{\lstate,\heap}\movesto\thread{}\tuple{\lstate',\heap'}$
%% $\tuple{\lstate,\heap}\movesto\thread{\lbl}\tuple{\lstate',\heap'}$, which
%% denote execution of method statements, labeled by $\lbl$. The label
%% $\lbl$ is empty, except
when the executed statement is annotated as
a linearization point, in which case it is labeled by the corresponding
operation (of form $\mname(\indata,\outdata)$).
%
%% We write $\proj\movesto\thread\lbl\proj'$ 
%% to denote that the statement labeled by $\lbl$ can be executed
%% from $\proj$, yielding $\proj'$.
%% Note that the next move of $\thread$ is uniquely determined
%% by $\proj$, since $\thread$ cannot access
%% the local states of other threads.
%

\paragraph{Programs.}
A {\it configuration} of a program
$\prog$ is a tuple
$\tuple{\athreads,\lstatemapping,\heap}$ where
$\athreads$ is a set of threads,
$\heap$ is a heap, and
$\lstatemapping$
%% is a {\it local state mapping} over $\athreads$ wrt.\ $\heap$ that
maps each thread $\thread\in\athreads$ to its
local state $\lstatemappingof\thread$
%% wrt.\ $\heap$.
%
%% We use $\confsetof\prog$ to denote the set of configurations
%% of $\prog$.
%
%% The initial configuration
%% %% $\initconfof\prog\in\confsetof\prog$ 
%% $\initconfof\prog$ 
%% is the pair
%% $\tuple{\initlstatemapping,\initheap}$,
%% where
%% %% $\initheap$ is the initial heap, and
%% $\initlstatemappingof\thread=\idlestate$ for each
%% $\thread\in\athreads$,
%% i.e.,
%% $\prog$ starts its execution from a configuration with an initial
%% heap, and with each thread in its initial local state.
%
The behavior of a program $\prog$ can be formalized by a transition relation
$\movesto\prog{}$ where each step corresponds to one move of a single thread.
I.e., 
there is a transition of form
$\tuple{\athreads,\lstatemapping,\heap}
\movesto\prog{\lbl}
\tuple{\athreads,\lstatemapping[\thread\leftarrow\lstate'],\heap'}$
whenever the transition relation $\movesto\thread{}$ has a transition
$\tuple{\lstate,\heap}\movesto\thread\lbl\tuple{\lstate',\heap'}$.
%% where the label $\lbl$ is either a call or return action or the empty label.
%
%% Note that the only visible transitions are those
%% corresponding to call and return actions.


We use $\system=\prog\compose\observer$ to denote the cross-product of
 a program  $\prog$ and observer $\observer$.
%
%% The initial configuration of $\system$ is
%% $\tuple{\initconfof\prog,\oinitstate}$.
Transitions of $\system$ are of the form
$\tuple{\pconf,\ostate},\movesto{\system}{},\tuple{{\pconf}',\ostate'}$,
obtained from a transition
$\pconf \movesto\prog{\lbl} {\pconf}'$ of the program with some (possibly empty)
label $\lbl$, where the observer makes a transition if it can perform a matching
transition.
%% $\tuple{\ostate,\returntwo{\mname}{l},\ostate'}$.

This construction reduces the problem of verifying that the represented
concurrent algorithm is a linearizable implementation of a data structure,
whose semantics is captured by the observer, to checking that
$\system$ cannot reach a configuration $\tuple{\pconf,\oaccstate}$
with an accepting observer state.

%% A {\it history} of $\prog$ is the sequence of call and return actions in
%% some sequence of transitions of $\movesto\prog{}$ from the initial configuration.
%% We define $\historyof\prog$ to be the set of histories of $\prog$.

\todo[inline]{We may have to say that we check the trivial conditions that
  were checked by the monitor in SAS 16}




